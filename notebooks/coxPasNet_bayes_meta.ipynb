{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoxPASNet.coxpasnet.DataLoader import load_data, load_pathway\n",
    "from CoxPASNet.coxpasnet.Train import trainCoxPASNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from CoxPASNet.coxpasnet.Survival_CostFunc_CIndex import R_set, neg_par_log_likelihood, c_index\n",
    "\n",
    "import torch.optim as optim\n",
    "from bayesian_torch.models.dnn_to_bnn import dnn_to_bnn, get_kl_loss\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "''' Net Settings'''\n",
    "In_Nodes = 5567 ###number of genes\n",
    "Pathway_Nodes = 860 ###number of pathways\n",
    "Hidden_Nodes = 100 ###number of hidden nodes\n",
    "Out_Nodes = 30 ###number of hidden nodes in the last hidden layer\n",
    "''' Initialize '''\n",
    "Initial_Learning_Rate = [0.03, 0.00075] #[0.03, 0.01, 0.001, 0.00075]\n",
    "L2_Lambda = [0.1, 0.001]  #[0.1, 0.01, 0.005, 0.001]\n",
    "num_epochs = 10 #3000 ###for grid search\n",
    "Num_EPOCHS = 5 #20000 ###for training\n",
    "###sub-network setup\n",
    "Dropout_Rate = [0.7, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "''' load data and pathway '''\n",
    "pathway_mask = load_pathway(\"../data/pathway_mask.csv\", dtype)\n",
    "\n",
    "x_train, ytime_train, yevent_train, age_train = load_data(\"../data/train.csv\", dtype)\n",
    "x_valid, ytime_valid, yevent_valid, age_valid = load_data(\"../data/validation.csv\", dtype)\n",
    "x_test, ytime_test, yevent_test, age_test = load_data(\"../data/test.csv\", dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in Train:  tensor([4.7945], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dy/sjxgt5r91_s5m71lvl0gghh80000gn/T/ipykernel_68628/37188299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mL2_Lambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mInitial_Learning_Rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \t\tloss_train, loss_valid, c_index_tr, c_index_va = trainCoxPASNet(x_train, age_train, ytime_train, yevent_train, \\\n\u001b[0m\u001b[1;32m     14\u001b[0m                                                                                                                                 \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytime_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myevent_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathway_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                                                                                                 \u001b[0mIn_Nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPathway_Nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHidden_Nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOut_Nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/survival_analysis/CoxPASNet/coxpasnet/Train.py\u001b[0m in \u001b[0;36mtrainCoxPASNet\u001b[0;34m(train_x, train_age, train_ytime, train_yevent, eval_x, eval_age, eval_ytime, eval_yevent, pathway_mask, In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, Learning_Rate, L2, Num_Epochs, Dropout_Rate)\u001b[0m\n\u001b[1;32m     82\u001b[0m                                 \u001b[0mcopy_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                                 \u001b[0mcopy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                                 \u001b[0my_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_age\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                                 \u001b[0mloss_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_par_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ytime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_yevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                 \u001b[0mS_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/survival_analysis/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/survival_analysis/CoxPASNet/coxpasnet/Model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_1, x_2)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m###force the connections between gene layer and pathway layer w.r.t. 'pathway_mask'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpathway_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m###construct a small sub-network for training only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/survival_analysis/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/survival_analysis/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt_l2_loss = 0\n",
    "opt_lr_loss = 0\n",
    "opt_loss = torch.Tensor([float(\"Inf\")])\n",
    "###if gpu is being used\n",
    "if torch.cuda.is_available():\n",
    "\topt_loss = opt_loss.cuda()\n",
    "###\n",
    "opt_c_index_va = 0\n",
    "opt_c_index_tr = 0\n",
    "###grid search the optimal hyperparameters using train and validation data\n",
    "for l2 in L2_Lambda:\n",
    "\tfor lr in Initial_Learning_Rate:\n",
    "\t\tloss_train, loss_valid, c_index_tr, c_index_va = trainCoxPASNet(x_train, age_train, ytime_train, yevent_train, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tx_valid, age_valid, ytime_valid, yevent_valid, pathway_mask, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIn_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlr, l2, num_epochs, Dropout_Rate)\n",
    "\t\tif loss_valid < opt_loss:\n",
    "\t\t\topt_l2_loss = l2\n",
    "\t\t\topt_lr_loss = lr\n",
    "\t\t\topt_loss = loss_valid\n",
    "\t\t\topt_c_index_tr = c_index_tr\n",
    "\t\t\topt_c_index_va = c_index_va\n",
    "\t\tprint (\"L2: \", l2, \"LR: \", lr, \"Loss in Validation: \", loss_valid)\n",
    "\n",
    "\n",
    "\n",
    "###train Cox-PASNet with optimal hyperparameters using train data, and then evaluate the trained model with test data\n",
    "###Note that test data are only used to evaluate the trained Cox-PASNet\n",
    "loss_train, loss_test, c_index_tr, c_index_te = trainCoxPASNet(x_train, age_train, ytime_train, yevent_train, \\\n",
    "\t\t\t\t\t\t\tx_test, age_test, ytime_test, yevent_test, pathway_mask, \\\n",
    "\t\t\t\t\t\t\tIn_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \\\n",
    "\t\t\t\t\t\t\topt_lr_loss, opt_l2_loss, Num_EPOCHS, Dropout_Rate)\n",
    "print (\"Optimal L2: \", opt_l2_loss, \"Optimal LR: \", opt_lr_loss)\n",
    "print(\"C-index in Test: \", c_index_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesian_torch.layers as bayesian_layers\n",
    "from bayesian_torch.utils.util import get_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as distributions\n",
    "\n",
    "\n",
    "class BaseVariationalLayer_(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._dnn_to_bnn_flag = False\n",
    "\n",
    "    @property\n",
    "    def dnn_to_bnn_flag(self):\n",
    "        return self._dnn_to_bnn_flag\n",
    "\n",
    "    @dnn_to_bnn_flag.setter\n",
    "    def dnn_to_bnn_flag(self, value):\n",
    "        self._dnn_to_bnn_flag = value\n",
    "\n",
    "    def kl_div(self, mu_q, sigma_q, mu_p, sigma_p):\n",
    "        \"\"\"\n",
    "        Calculates kl divergence between two gaussians (Q || P)\n",
    "        Parameters:\n",
    "             * mu_q: torch.Tensor -> mu parameter of distribution Q\n",
    "             * sigma_q: torch.Tensor -> sigma parameter of distribution Q\n",
    "             * mu_p: float -> mu parameter of distribution P\n",
    "             * sigma_p: float -> sigma parameter of distribution P\n",
    "        returns torch.Tensor of shape 0\n",
    "        \"\"\"\n",
    "        kl = torch.log(sigma_p) - torch.log(\n",
    "            sigma_q) + (sigma_q**2 + (mu_q - mu_p)**2) / (2 *\n",
    "                                                          (sigma_p**2)) - 0.5\n",
    "        return kl.mean()\n",
    "    \n",
    "\n",
    "def bnn_linear_layer_cust(params, d):\n",
    "    # Get BNN layer\n",
    "    bnn_layer = LinearReparam(\n",
    "        in_features=d.in_features,\n",
    "        out_features=d.out_features,\n",
    "        prior_mean=params[\"prior_mu\"],\n",
    "        prior_variance=params[\"prior_sigma\"],\n",
    "        posterior_mu_init=params[\"posterior_mu_init\"],\n",
    "        posterior_rho_init=params[\"posterior_rho_init\"],\n",
    "        bias=d.bias is not None,\n",
    "    )\n",
    "    bnn_layer.dnn_to_bnn_flag = True\n",
    "    return bnn_layer\n",
    "\n",
    "def dnn_to_bnn_bcoxpas(m, bnn_prior_parameters):\n",
    "    for name, value in list(m._modules.items()):\n",
    "        if m._modules[name]._modules:\n",
    "            dnn_to_bnn_bcoxpas(m._modules[name], bnn_prior_parameters)\n",
    "        if name == \"sc1\":\n",
    "            pass\n",
    "        elif \"Linear\" in m._modules[name].__class__.__name__:\n",
    "            setattr(\n",
    "                m,\n",
    "                name,\n",
    "                bnn_linear_layer_cust(\n",
    "                    bnn_prior_parameters,\n",
    "                    m._modules[name]))\n",
    "        else:\n",
    "            pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LinearReparameterization(BaseVariationalLayer_):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 prior_mean=0,\n",
    "                 prior_variance=1,\n",
    "                 posterior_mu_init=0,\n",
    "                 posterior_rho_init=-3.0,\n",
    "                 bias=True):\n",
    "        \"\"\"\n",
    "        Implements Linear layer with reparameterization trick.\n",
    "        Inherits from bayesian_torch.layers.BaseVariationalLayer_\n",
    "        Parameters:\n",
    "            in_features: int -> size of each input sample,\n",
    "            out_features: int -> size of each output sample,\n",
    "            prior_mean: float -> mean of the prior arbitrary distribution to be used on the complexity cost,\n",
    "            prior_variance: float -> variance of the prior arbitrary distribution to be used on the complexity cost,\n",
    "            posterior_mu_init: float -> init trainable mu parameter representing mean of the approximate posterior,\n",
    "            posterior_rho_init: float -> init trainable rho parameter representing the sigma of the approximate posterior through softplus function,\n",
    "            bias: bool -> if set to False, the layer will not learn an additive bias. Default: True,\n",
    "        \"\"\"\n",
    "        super(LinearReparameterization, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.prior_mean = prior_mean\n",
    "        self.prior_variance = prior_variance\n",
    "        self.posterior_mu_init = posterior_mu_init,  # mean of weight\n",
    "        # variance of weight --> sigma = log (1 + exp(rho))\n",
    "        self.posterior_rho_init = posterior_rho_init,\n",
    "        self.bias = bias\n",
    "\n",
    "        self.mu_weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.rho_weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer('eps_weight',\n",
    "                             torch.Tensor(out_features, in_features),\n",
    "                             persistent=False)\n",
    "        self.register_buffer('prior_weight_mu',\n",
    "                             torch.Tensor(out_features, in_features),\n",
    "                             persistent=False)\n",
    "        self.register_buffer('prior_weight_sigma',\n",
    "                             torch.Tensor(out_features, in_features),\n",
    "                             persistent=False)\n",
    "        if bias:\n",
    "            self.mu_bias = Parameter(torch.Tensor(out_features))\n",
    "            self.rho_bias = Parameter(torch.Tensor(out_features))\n",
    "            self.register_buffer(\n",
    "                'eps_bias',\n",
    "                torch.Tensor(out_features),\n",
    "                persistent=False)\n",
    "            self.register_buffer(\n",
    "                'prior_bias_mu',\n",
    "                torch.Tensor(out_features),\n",
    "                persistent=False)\n",
    "            self.register_buffer('prior_bias_sigma',\n",
    "                                 torch.Tensor(out_features),\n",
    "                                 persistent=False)\n",
    "        else:\n",
    "            self.register_buffer('prior_bias_mu', None, persistent=False)\n",
    "            self.register_buffer('prior_bias_sigma', None, persistent=False)\n",
    "            self.register_parameter('mu_bias', None)\n",
    "            self.register_parameter('rho_bias', None)\n",
    "            self.register_buffer('eps_bias', None, persistent=False)\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        self.prior_weight_mu.fill_(self.prior_mean)\n",
    "        self.prior_weight_sigma.fill_(self.prior_variance)\n",
    "\n",
    "        self.mu_weight.data.normal_(mean=self.posterior_mu_init[0], std=0.1)\n",
    "        self.rho_weight.data.normal_(mean=self.posterior_rho_init[0], std=0.1)\n",
    "        if self.mu_bias is not None:\n",
    "            self.prior_bias_mu.fill_(self.prior_mean)\n",
    "            self.prior_bias_sigma.fill_(self.prior_variance)\n",
    "            self.mu_bias.data.normal_(mean=self.posterior_mu_init[0], std=0.1)\n",
    "            self.rho_bias.data.normal_(mean=self.posterior_rho_init[0],\n",
    "                                       std=0.1)\n",
    "\n",
    "    def kl_loss(self):\n",
    "        sigma_weight = torch.log1p(torch.exp(self.rho_weight))\n",
    "        kl = self.kl_div(\n",
    "            self.mu_weight,\n",
    "            sigma_weight,\n",
    "            self.prior_weight_mu,\n",
    "            self.prior_weight_sigma)\n",
    "        if self.mu_bias is not None:\n",
    "            sigma_bias = torch.log1p(torch.exp(self.rho_bias))\n",
    "            kl += self.kl_div(self.mu_bias, sigma_bias,\n",
    "                              self.prior_bias_mu, self.prior_bias_sigma)\n",
    "        return kl\n",
    "\n",
    "    def forward(self, input, return_kl=True):\n",
    "        if self.dnn_to_bnn_flag:\n",
    "            return_kl = False\n",
    "        sigma_weight = torch.log1p(torch.exp(self.rho_weight))\n",
    "        weight = self.mu_weight + \\\n",
    "            (sigma_weight * self.eps_weight.data.normal_())\n",
    "        if return_kl:\n",
    "            kl_weight = self.kl_div(self.mu_weight, sigma_weight,\n",
    "                                    self.prior_weight_mu, self.prior_weight_sigma)\n",
    "        bias = None\n",
    "\n",
    "        if self.mu_bias is not None:\n",
    "            sigma_bias = torch.log1p(torch.exp(self.rho_bias))\n",
    "            bias = self.mu_bias + (sigma_bias * self.eps_bias.data.normal_())\n",
    "            if return_kl:\n",
    "                kl_bias = self.kl_div(self.mu_bias, sigma_bias, self.prior_bias_mu,\n",
    "                                      self.prior_bias_sigma)\n",
    "\n",
    "        out = F.linear(input, weight, bias)\n",
    "        if return_kl:\n",
    "            if self.mu_bias is not None:\n",
    "                kl = kl_weight + kl_bias\n",
    "            else:\n",
    "                kl = kl_weight\n",
    "\n",
    "            return out, kl\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, Parameter\n",
    "\n",
    "\n",
    "class LinearReparam(BaseVariationalLayer_):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 prior_means,\n",
    "                 prior_variances,\n",
    "                 posterior_mu_init=0,\n",
    "                 posterior_rho_init=-3.0,\n",
    "                 bias=True):\n",
    "        \"\"\"\n",
    "        Implements Linear layer with reparameterization trick.\n",
    "        Inherits from bayesian_torch.layers.BaseVariationalLayer_\n",
    "        Parameters:\n",
    "            in_features: int -> size of each input sample,\n",
    "            out_features: int -> size of each output sample,\n",
    "            prior_mean: float -> mean of the prior arbitrary distribution to be used on the complexity cost,\n",
    "            prior_variance: float -> variance of the prior arbitrary distribution to be used on the complexity cost,\n",
    "            posterior_mu_init: float -> init trainable mu parameter representing mean of the approximate posterior,\n",
    "            posterior_rho_init: float -> init trainable rho parameter representing the sigma of the approximate posterior through softplus function,\n",
    "            bias: bool -> if set to False, the layer will not learn an additive bias. Default: True,\n",
    "        \"\"\"\n",
    "        super(LinearReparam, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.prior_means = prior_means\n",
    "        self.prior_variances = prior_variances\n",
    "        self.posterior_mu_init = posterior_mu_init,  # mean of weight\n",
    "        # variance of weight --> sigma = log (1 + exp(rho))\n",
    "        self.posterior_rho_init = posterior_rho_init,\n",
    "        self.bias = bias\n",
    "\n",
    "        self.mu_weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.rho_weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer('eps_weight',\n",
    "                             torch.Tensor(out_features, in_features),\n",
    "                             persistent=False)\n",
    "        self.register_buffer('prior_weight_mu',\n",
    "                             torch.Tensor(out_features, in_features),\n",
    "                             persistent=False)\n",
    "        self.register_buffer('prior_weight_sigma',\n",
    "                             torch.Tensor(out_features, in_features),\n",
    "                             persistent=False)\n",
    "        if bias:\n",
    "            self.mu_bias = Parameter(torch.Tensor(out_features))\n",
    "            self.rho_bias = Parameter(torch.Tensor(out_features))\n",
    "            self.register_buffer(\n",
    "                'eps_bias',\n",
    "                torch.Tensor(out_features),\n",
    "                persistent=False)\n",
    "            self.register_buffer(\n",
    "                'prior_bias_mu',\n",
    "                torch.Tensor(out_features),\n",
    "                persistent=False)\n",
    "            self.register_buffer('prior_bias_sigma',\n",
    "                                 torch.Tensor(out_features),\n",
    "                                 persistent=False)\n",
    "        else:\n",
    "            self.register_buffer('prior_bias_mu', None, persistent=False)\n",
    "            self.register_buffer('prior_bias_sigma', None, persistent=False)\n",
    "            self.register_parameter('mu_bias', None)\n",
    "            self.register_parameter('rho_bias', None)\n",
    "            self.register_buffer('eps_bias', None, persistent=False)\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        self.prior_weight_mu = torch.from_numpy(self.prior_means)\n",
    "        self.prior_weight_sigma = torch.from_numpy(self.prior_variances)\n",
    "\n",
    "        self.mu_weight.data.normal_(mean=self.posterior_mu_init[0], std=0.1)\n",
    "        self.rho_weight.data.normal_(mean=self.posterior_rho_init[0], std=0.1)\n",
    "        if self.mu_bias is not None:\n",
    "            self.prior_bias_mu =torch.from_numpy(np.mean(self.prior_means,axis = 1))\n",
    "            self.prior_bias_sigma = torch.from_numpy(np.mean(self.prior_variances,axis =1 ))\n",
    "            self.mu_bias.data.normal_(mean=self.posterior_mu_init[0], std=0.1)\n",
    "            self.rho_bias.data.normal_(mean=self.posterior_rho_init[0],\n",
    "                                       std=0.1)\n",
    "    def kl_loss(self):\n",
    "        sigma_weight = torch.log1p(torch.exp(self.rho_weight))\n",
    "        kl = self.kl_div(\n",
    "            self.mu_weight,\n",
    "            sigma_weight,\n",
    "            self.prior_weight_mu,\n",
    "            self.prior_weight_sigma)\n",
    "        if self.mu_bias is not None:\n",
    "            sigma_bias = torch.log1p(torch.exp(self.rho_bias))\n",
    "            kl += self.kl_div(self.mu_bias, sigma_bias,\n",
    "                              self.prior_bias_mu, self.prior_bias_sigma)\n",
    "        return kl\n",
    "\n",
    "    def forward(self, input, return_kl=True):\n",
    "        if self.dnn_to_bnn_flag:\n",
    "            return_kl = False\n",
    "        sigma_weight = torch.log1p(torch.exp(self.rho_weight))\n",
    "        weight = self.mu_weight + \\\n",
    "            (sigma_weight * self.eps_weight.data.normal_())\n",
    "        if return_kl:\n",
    "            kl_weight = self.kl_div(self.mu_weight, sigma_weight,\n",
    "                                    self.prior_weight_mu, self.prior_weight_sigma)\n",
    "        bias = None\n",
    "\n",
    "        if self.mu_bias is not None:\n",
    "            sigma_bias = torch.log1p(torch.exp(self.rho_bias))\n",
    "            bias = self.mu_bias + (sigma_bias * self.eps_bias.data.normal_())\n",
    "            if return_kl:\n",
    "                kl_bias = self.kl_div(self.mu_bias, sigma_bias, self.prior_bias_mu,\n",
    "                                      self.prior_bias_sigma)\n",
    "\n",
    "        out = F.linear(input, weight, bias)\n",
    "        if return_kl:\n",
    "            if self.mu_bias is not None:\n",
    "                kl = kl_weight + kl_bias\n",
    "            else:\n",
    "                kl = kl_weight\n",
    "\n",
    "            return out, kl\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer = LinearReparam(\n",
    "        in_features=1,\n",
    "        out_features=2,\n",
    "        prior_means=np.array([[0.],[0.]]),\n",
    "        prior_variances=np.array([[0.1],[0.1]]),\n",
    "        posterior_mu_init=0.5,\n",
    "        posterior_rho_init=-3.0,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "#test_layer.kl_loss()\n",
    "\n",
    "#inp = torch.tensor([[1]],dtype = torch.float)\n",
    "#r = test_layer.forward(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_lay_normal = LinearReparameterization(\n",
    "        in_features=1,\n",
    "        out_features=2,\n",
    "        prior_mean=0,\n",
    "        prior_variance=0.00000001,\n",
    "        posterior_mu_init=0.5,\n",
    "        posterior_rho_init=-3,\n",
    "        bias=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lay_normal.posterior_mu_init[0]\n",
    "test_lay_normal.mu_weight\n",
    "test_lay_normal.prior_weight_sigma\n",
    "test_lay_normal.prior_bias_mu\n",
    "test_lay_normal.prior_weight_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000], dtype=torch.float64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Tmr check why these layers dont yield the same result. Maybe assign some variables\n",
    "# and call get kl function to see if that one works\n",
    "\n",
    "test_layer.prior_bias_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp = torch.tensor([[1]],dtype = torch.float)\n",
    "\n",
    "res_normal_1 = []\n",
    "res_normal_2 = []\n",
    "res_normal_kl = []\n",
    "res_new_1 = []\n",
    "res_new_2 = []\n",
    "res_new_kl = []\n",
    "\n",
    "for i in range(10000):\n",
    "    norm_res = test_lay_normal.forward(inp)\n",
    "    new_res = test_layer.forward(inp)\n",
    "\n",
    "    res_normal_1.append(norm_res[0].detach().numpy()[0][0])\n",
    "    res_normal_2.append(norm_res[0].detach().numpy()[0][1])\n",
    "    res_normal_kl.append(norm_res[1].detach().numpy())\n",
    "\n",
    "    res_new_1.append(new_res[0].detach().numpy()[0][0])\n",
    "    res_new_2.append(new_res[0].detach().numpy()[0][1])\n",
    "    res_new_kl.append(new_res[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0.],[0.]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.full((3,1),0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Bay_TestNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, In_Nodes, Hidden_Nodes, Out_Nodes,mean=0.,variance=.1):\n",
    "        super(Bay_TestNet, self).__init__()\n",
    "        #self.tanh = nn.Tanh()\n",
    "        self.l1 = LinearReparam(in_features=In_Nodes,\n",
    "                                out_features=Out_Nodes,\n",
    "                                prior_means=np.full((Out_Nodes,In_Nodes),mean),\n",
    "                                prior_variances=np.full((Out_Nodes,In_Nodes),variance),\n",
    "                                posterior_mu_init=0.5,\n",
    "                                posterior_rho_init=-3.0,\n",
    "                                bias=False,\n",
    "                                )\n",
    "        \n",
    "        '''\n",
    "        self.l2 = LinearReparam(in_features=Hidden_Nodes,\n",
    "                                out_features=Hidden_Nodes,\n",
    "                                prior_means=np.full((Hidden_Nodes,Hidden_Nodes),mean),\n",
    "                                prior_variances=np.full((Hidden_Nodes,Hidden_Nodes),variance),\n",
    "                                posterior_mu_init=0.5,\n",
    "                                posterior_rho_init=-3.0,\n",
    "                                bias=False,\n",
    "                                )\n",
    "        \n",
    "        self.l3 = LinearReparam(in_features=Hidden_Nodes,\n",
    "                                out_features=Hidden_Nodes,\n",
    "                                prior_means=np.full((Hidden_Nodes,Hidden_Nodes),mean),\n",
    "                                prior_variances=np.full((Hidden_Nodes,Hidden_Nodes),variance),\n",
    "                                posterior_mu_init=0.5,\n",
    "                                posterior_rho_init=-3.0,\n",
    "                                bias=False,\n",
    "                                )\n",
    "        self.l4 = LinearReparam(in_features=Hidden_Nodes,\n",
    "                                out_features=Out_Nodes,\n",
    "                                prior_means=np.full((Out_Nodes,Hidden_Nodes),mean),\n",
    "                                prior_variances=np.full((Out_Nodes,Hidden_Nodes),variance),\n",
    "                                posterior_mu_init=0.5,\n",
    "                                posterior_rho_init=-3.0,\n",
    "                                bias=False,\n",
    "                                )\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        \n",
    "        lin_pred = self.l1(x)\n",
    "\n",
    "        return lin_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainbaynet(train_x, train_age, train_ytime, train_yevent, \\\n",
    "\t\t\teval_x, eval_age, eval_ytime, eval_yevent, pathway_mask, \\\n",
    "\t\t\tIn_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \\\n",
    "\t\t\tLearning_Rate, Num_Epochs,bnn_prior_params):\n",
    "\n",
    "    net = Bay_CPASNet(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask)\n",
    "    dnn_to_bnn_bcoxpas(net, bnn_prior_params)\n",
    "    \n",
    "    ###\n",
    "    ###optimizer\n",
    "    opt = optim.Adam(net.parameters(), lr=Learning_Rate)\n",
    "\n",
    "    for epoch in range(Num_Epochs+1):\n",
    "        net.train()\n",
    "        opt.zero_grad() ###reset gradients to zeros\n",
    "\n",
    "        pred = net(train_x, train_age) ###Forward\n",
    "        ce_loss = neg_par_log_likelihood(pred, train_ytime, train_yevent)\n",
    "        kl = get_kl_loss(net)\n",
    "        loss = ce_loss + kl\n",
    "        loss.backward() ###calculate gradients\n",
    "        opt.step()\n",
    "        net.sc1.weight.data = net.sc1.weight.data.mul(net.pathway_mask) ###force the connections between gene layer and pathway layer\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                net.train()\n",
    "                train_output_mc = []\n",
    "                for mc_run in range(10):\n",
    "                    output = net(train_x, train_age)\n",
    "                    train_output_mc.append(output)\n",
    "                    outputs = torch.stack(train_output_mc)\n",
    "                train_pred = outputs.mean(dim=0)\n",
    "                train_loss = neg_par_log_likelihood(train_pred, train_ytime, train_yevent).view(1,)\n",
    "\n",
    "                eval_output_mc = []\n",
    "                for mc_run in range(10):\n",
    "                    output = net(eval_x, eval_age)\n",
    "                    eval_output_mc.append(output)\n",
    "                    eval_outputs = torch.stack(eval_output_mc)\n",
    "                eval_pred = eval_outputs.mean(dim=0)\n",
    "                eval_loss = neg_par_log_likelihood(eval_pred, eval_ytime, eval_yevent).view(1,)\n",
    "\n",
    "                train_cindex = c_index(train_pred, train_ytime, train_yevent)\n",
    "                eval_cindex = c_index(eval_pred, eval_ytime, eval_yevent)\n",
    "                print(f\"Epoch: {epoch}, Train Loss: {train_loss},Eval Loss: {eval_loss}, \"\n",
    "                      f\" Train Cindex: {train_cindex}, Eval Cindex: {eval_cindex}\")\n",
    "\n",
    "    return (train_loss, eval_loss, train_cindex, eval_cindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args,\n",
    "          train_loader,\n",
    "          model,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          epoch,\n",
    "          tb_writer=None):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    mses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cpu()\n",
    "        input_var = input.cpu()\n",
    "        target_var = target\n",
    "\n",
    "\n",
    "        # compute output\n",
    "        output_ = []\n",
    "        kl_ = []\n",
    "        for mc_run in range(args.num_mc):\n",
    "            output, kl = model(input_var)\n",
    "            output_.append(output)\n",
    "            kl_.append(kl)\n",
    "        output = torch.mean(torch.stack(output_), dim=0)\n",
    "        kl = torch.mean(torch.stack(kl_), dim=0)\n",
    "        cross_entropy_loss = criterion(output, target_var)\n",
    "        scaled_kl = kl / args.batch_size \n",
    "        #ELBO loss\n",
    "        loss = cross_entropy_loss + scaled_kl\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        mse = criterion(output.data, target)\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        mses.update(mse.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Mses {mses.val:.3f} ({mses.avg:.3f})'.format(\n",
    "                      epoch,\n",
    "                      i,\n",
    "                      len(train_loader),\n",
    "                      batch_time=batch_time,\n",
    "                      data_time=data_time,\n",
    "                      loss=losses,\n",
    "                      mses=mses))\n",
    "\n",
    "        if tb_writer is not None:\n",
    "            tb_writer.add_scalar('train/cross_entropy_loss',\n",
    "                                 cross_entropy_loss.item(), epoch)\n",
    "            tb_writer.add_scalar('train/kl_div', scaled_kl.item(), epoch)\n",
    "            tb_writer.add_scalar('train/elbo_loss', loss.item(), epoch)\n",
    "            tb_writer.add_scalar('train/accuracy', prec1.item(), epoch)\n",
    "            tb_writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(args, val_loader, model, criterion, epoch, tb_writer=None):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    errors = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cpu()\n",
    "            input_var = input.cpu()\n",
    "            target_var = target.cpu()\n",
    "\n",
    "\n",
    "            # compute output\n",
    "            output_ = []\n",
    "            kl_ = []\n",
    "            for mc_run in range(args.num_mc):\n",
    "                output, kl = model(input_var)\n",
    "                output_.append(output)\n",
    "                kl_.append(kl)\n",
    "            output = torch.mean(torch.stack(output_), dim=0)\n",
    "            kl = torch.mean(torch.stack(kl_), dim=0)\n",
    "            cross_entropy_loss = criterion(output, target_var)\n",
    "            scaled_kl = kl / args.batch_size \n",
    "            #ELBO loss\n",
    "            loss = cross_entropy_loss + scaled_kl\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            errors.update(error.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Error{error.val:.3f} ({error.avg:.3f})'.format(\n",
    "                          i,\n",
    "                          len(val_loader),\n",
    "                          batch_time=batch_time,\n",
    "                          loss=losses,\n",
    "                          error=errors))\n",
    "        '''\n",
    "            if tb_writer is not None:\n",
    "                tb_writer.add_scalar('val/cross_entropy_loss',\n",
    "                                     cross_entropy_loss.item(), epoch)\n",
    "                tb_writer.add_scalar('val/kl_div', scaled_kl.item(), epoch)\n",
    "                tb_writer.add_scalar('val/elbo_loss', loss.item(), epoch)\n",
    "                tb_writer.add_scalar('val/accuracy', prec1.item(), epoch)\n",
    "                tb_writer.flush()\n",
    "        '''\n",
    "    print(' * Error {error.avg:.3f}'.format(error=errors))\n",
    "\n",
    "    return errors.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model,criterion, val_loader):\n",
    "    pred_probs_mc = []\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    output_list = []\n",
    "    labels_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        begin = time.time()\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.cpu(), target.cpu()\n",
    "            output_mc = []\n",
    "            for mc_run in range(args.num_mc):\n",
    "                output, _ = model.forward(data)\n",
    "                output_mc.append(output)\n",
    "            output_ = torch.mean(torch.stack(output_mc),dim=0)\n",
    "            output_list.append(output_)\n",
    "        end = time.time()\n",
    "\n",
    "        print('Test Error:',\n",
    "              (criterion(output_list,target)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    def __init__(self,num_mc,batch_size,print_freq,epochs,mode,lr,workers,model_name,save_dir):\n",
    "        self.num_mc = num_mc\n",
    "        self.batch_size = batch_size\n",
    "        self.print_freq = print_freq\n",
    "        self.epochs = epochs\n",
    "        self.mode = mode\n",
    "        self.lr = lr\n",
    "        self.workers = workers\n",
    "        self.model_name = model_name\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "args = arguments(200,1,500,3,\"test\",0.00009,0,\"test_model_no_noise\",\"model_checkpoints\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(size = 1500)\n",
    "x2 = np.random.normal(size = 1500)\n",
    "X = np.stack((x1,x2),axis = -1)\n",
    "y = x1 + x2  \n",
    "#+ np.random.normal(0,0.01,size = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:1000]\n",
    "y_train = y[0:1000]\n",
    "X_test = X[1000:1500]\n",
    "y_test = y[1000:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, y, scale_data=False):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X).float()\n",
    "            self.y = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "test_dataset = Dataset(X_test,y_test)\n",
    "#trainloader = torch.utils.data.DataLoader(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dy/sjxgt5r91_s5m71lvl0gghh80000gn/T/ipykernel_70116/3916228922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                     map_location=torch.device('cpu'))\n\u001b[1;32m     76\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dy/sjxgt5r91_s5m71lvl0gghh80000gn/T/ipykernel_70116/2939653516.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, model, criterion, val_loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         print('Test Error:',\n\u001b[0;32m---> 21\u001b[0;31m               (criterion(output_list,target)))\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/survival_analysis/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/survival_analysis/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/survival_analysis/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3249\u001b[0m             \u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3250\u001b[0m         )\n\u001b[0;32m-> 3251\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3252\u001b[0m         warnings.warn(\n\u001b[1;32m   3253\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "model = Bay_TestNet(2,3,1)\n",
    "model.cpu()\n",
    "\n",
    "best_pred = 0\n",
    "\n",
    "tb_writer = None\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( dataset,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=args.workers,\n",
    "                                            pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                         batch_size=args.batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=args.workers,\n",
    "                                         pin_memory=True)\n",
    "\n",
    "criterion = nn.MSELoss().cpu()\n",
    "\n",
    "  \n",
    "'''\n",
    "\n",
    "    if args.evaluate:\n",
    "        validate(val_loader, model, criterion)\n",
    "        return\n",
    "'''\n",
    "if args.mode == 'train':\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "            lr = args.lr\n",
    "            if (epoch >= 80 and epoch < 120):\n",
    "                lr = 0.1 * args.lr\n",
    "            elif (epoch >= 120 and epoch < 160):\n",
    "                lr = 0.01 * args.lr\n",
    "            elif (epoch >= 160 and epoch < 180):\n",
    "                lr = 0.001 * args.lr\n",
    "            elif (epoch >= 180):\n",
    "                lr = 0.0005 * args.lr\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "            # train for one epoch\n",
    "            print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "            train(args, train_loader, model, criterion, optimizer, epoch,\n",
    "                  tb_writer)\n",
    "\n",
    "            val_score = validate(args, val_loader, model, criterion, epoch,\n",
    "                             tb_writer)\n",
    "\n",
    "            is_best = val_score >= best_pred\n",
    "            best_val = max(val_score, best_pred)\n",
    "\n",
    "            if is_best:\n",
    "                save_checkpoint(\n",
    "                    {\n",
    "                    'epoch': epoch + 1,\n",
    "                     'state_dict': model.state_dict(),\n",
    "                    'best_val': best_val,\n",
    "                   },\n",
    "                    is_best,\n",
    "                    filename=os.path.join(\n",
    "                        args.save_dir,\n",
    "                       'bayesian_{}.pth'.format(args.model_name)))\n",
    "\n",
    "elif args.mode == 'test':\n",
    "    checkpoint_file = args.save_dir + '/bayesian_{}.pth'.format(\n",
    "            args.model_name)\n",
    "    if torch.cuda.is_available():\n",
    "            checkpoint = torch.load(checkpoint_file)\n",
    "    else:\n",
    "            checkpoint = torch.load(checkpoint_file,\n",
    "                                    map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    evaluate(args, model, criterion,val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sth still seems to be wrong with the error function or train method -> increased steadily\n",
    "#while training while val loss decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_file = args.save_dir + '/bayesian_{}.pth'.format(\n",
    "            args.model_name)\n",
    "checkpoint = torch.load(checkpoint_file)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_dataset = Dataset(np.array([[0.2,0.1]]),np.array([[0.3]]))\n",
    "ttest_loader = torch.utils.data.DataLoader(ttest_dataset,\n",
    "                                         batch_size=args.batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=args.workers,\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_mc = []\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "output_list = []\n",
    "labels_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        begin = time.time()\n",
    "        for data, target in ttest_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            else:\n",
    "                data, target = data.cpu(), target.cpu()\n",
    "            output_mc = []\n",
    "            for mc_run in range(1000):\n",
    "                output, _ = model.forward(data)\n",
    "                output_mc.append(output)\n",
    "            output_ = torch.stack(output_mc)\n",
    "            output_list.append(output_)\n",
    "        end = time.time()\n",
    "        output = torch.stack(output_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0111]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(output_, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_net = Bay_TestNet(2,3,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04336347"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.tensor([[0.5,0.5]],dtype = torch.float)\n",
    "res = []\n",
    "\n",
    "for i in range(50000):\n",
    "    res.append(model(inp)[0].detach().cpu().numpy()[0][0])\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1276]], grad_fn=<MmBackward0>),\n",
       " tensor(0.0965, dtype=torch.float64, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0395]], grad_fn=<MmBackward0>),\n",
       " tensor(14.7322, dtype=torch.float64, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bay_net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Bay_CPASNet(nn.Module):\n",
    "\tdef __init__(self, In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, Pathway_Mask):\n",
    "\t\tsuper(Bay_CPASNet, self).__init__()\n",
    "\t\tself.tanh = nn.Tanh()\n",
    "\t\tself.pathway_mask = Pathway_Mask\n",
    "\t\t###gene layer --> pathway layer\n",
    "\t\tself.sc1 = nn.Linear(In_Nodes, Pathway_Nodes)\n",
    "\t\t###pathway layer --> hidden layer\n",
    "\t\tself.sc2 = nn.Linear(Pathway_Nodes, Hidden_Nodes)\n",
    "\t\t###hidden layer --> hidden layer 2\n",
    "\t\tself.sc3 = nn.Linear(Hidden_Nodes, Out_Nodes, bias=False)\n",
    "\t\t###hidden layer 2 + age --> Cox layer\n",
    "\t\tself.sc4 = nn.Linear(Out_Nodes+1, 1, bias = False)\n",
    "\t\tself.sc4.weight.data.uniform_(-0.001, 0.001)\n",
    "\n",
    "\tdef forward(self, x_1, x_2):\n",
    "\t\t###force the connections between gene layer and pathway layer w.r.t. 'pathway_mask'\n",
    "\t\tself.sc1.weight.data = self.sc1.weight.data.mul(self.pathway_mask)\n",
    "\t\tx_1 = self.tanh(self.sc1(x_1))\n",
    "\t\tx_1 = self.tanh(self.sc2(x_1))\n",
    "\t\tx_1 = self.tanh(self.sc3(x_1))\n",
    "\t\t###combine age with hidden layer 2\n",
    "\t\tx_cat = torch.cat((x_1, x_2), 1)\n",
    "\t\tlin_pred = self.sc4(x_cat)\n",
    "\n",
    "\t\treturn lin_pred\n",
    "\n",
    "def trainBayCoxPASNet(train_x, train_age, train_ytime, train_yevent, \\\n",
    "\t\t\teval_x, eval_age, eval_ytime, eval_yevent, pathway_mask, \\\n",
    "\t\t\tIn_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \\\n",
    "\t\t\tLearning_Rate, Num_Epochs,bnn_prior_params):\n",
    "\n",
    "    net = Bay_CPASNet(In_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, pathway_mask)\n",
    "    dnn_to_bnn_bcoxpas(net, bnn_prior_params)\n",
    "    \n",
    "    ###\n",
    "    ###optimizer\n",
    "    opt = optim.Adam(net.parameters(), lr=Learning_Rate)\n",
    "\n",
    "    for epoch in range(Num_Epochs+1):\n",
    "        net.train()\n",
    "        opt.zero_grad() ###reset gradients to zeros\n",
    "\n",
    "        pred = net(train_x, train_age) ###Forward\n",
    "        ce_loss = neg_par_log_likelihood(pred, train_ytime, train_yevent)\n",
    "        kl = get_kl_loss(net)\n",
    "        loss = ce_loss + kl\n",
    "        loss.backward() ###calculate gradients\n",
    "        opt.step()\n",
    "        net.sc1.weight.data = net.sc1.weight.data.mul(net.pathway_mask) ###force the connections between gene layer and pathway layer\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                net.train()\n",
    "                train_output_mc = []\n",
    "                for mc_run in range(10):\n",
    "                    output = net(train_x, train_age)\n",
    "                    train_output_mc.append(output)\n",
    "                    outputs = torch.stack(train_output_mc)\n",
    "                train_pred = outputs.mean(dim=0)\n",
    "                train_loss = neg_par_log_likelihood(train_pred, train_ytime, train_yevent).view(1,)\n",
    "\n",
    "                eval_output_mc = []\n",
    "                for mc_run in range(10):\n",
    "                    output = net(eval_x, eval_age)\n",
    "                    eval_output_mc.append(output)\n",
    "                    eval_outputs = torch.stack(eval_output_mc)\n",
    "                eval_pred = eval_outputs.mean(dim=0)\n",
    "                eval_loss = neg_par_log_likelihood(eval_pred, eval_ytime, eval_yevent).view(1,)\n",
    "\n",
    "                train_cindex = c_index(train_pred, train_ytime, train_yevent)\n",
    "                eval_cindex = c_index(eval_pred, eval_ytime, eval_yevent)\n",
    "                print(f\"Epoch: {epoch}, Train Loss: {train_loss},Eval Loss: {eval_loss}, \"\n",
    "                      f\" Train Cindex: {train_cindex}, Eval Cindex: {eval_cindex}\")\n",
    "\n",
    "    return (train_loss, eval_loss, train_cindex, eval_cindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: tensor([4.8027]),Eval Loss: tensor([3.5799]),  Train Cindex: 0.629298210144043, Eval Cindex: 0.6694067716598511\n"
     ]
    }
   ],
   "source": [
    "const_bnn_prior_parameters = {\n",
    "        \"prior_mu\": 0.0,\n",
    "        \"prior_sigma\": 1.0,\n",
    "        \"posterior_mu_init\": 0.0,\n",
    "        \"posterior_rho_init\": -3.0,\n",
    "        \"type\": \"Reparameterization\",  # Flipout or Reparameterization\n",
    "        \"moped_enable\": True,  # True to initialize mu/sigma from the pretrained dnn weights\n",
    "        \"moped_delta\": 0.5,\n",
    "}\n",
    "    \n",
    "opt_lr_loss = 0.01\n",
    "loss_train_bnn, loss_test_bnn, c_index_tr_bnn, c_index_te_bnn = trainBayCoxPASNet(x_train, age_train, ytime_train, yevent_train, \\\n",
    "\t\t\t\t\t\t\tx_test, age_test, ytime_test, yevent_test, pathway_mask, \\\n",
    "\t\t\t\t\t\t\tIn_Nodes, Pathway_Nodes, Hidden_Nodes, Out_Nodes, \\\n",
    "\t\t\t\t\t\t\topt_lr_loss, Num_EPOCHS,const_bnn_prior_parameters)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dy/sjxgt5r91_s5m71lvl0gghh80000gn/T/ipykernel_62958/583084179.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mage_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/survival_analysis/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dy/sjxgt5r91_s5m71lvl0gghh80000gn/T/ipykernel_62958/3308764857.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_1, x_2)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;31m###combine age with hidden layer 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mx_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mlin_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "net(x_train[0:2],age_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dy/sjxgt5r91_s5m71lvl0gghh80000gn/T/ipykernel_62958/3464743083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mage_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "torch.cat((x_train[0][None:],age_train[0][None:]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_train[0][None:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
