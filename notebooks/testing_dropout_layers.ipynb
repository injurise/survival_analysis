{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from CoxPASNet.coxpasnet.DataLoader import load_data, load_pathway\n",
    "from CoxPASNet.coxpasnet.Train import trainCoxPASNet\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from CoxPASNet.coxpasnet.Survival_CostFunc_CIndex import R_set, neg_par_log_likelihood, c_index\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "from src.data_prep.torch_datasets import cpath_dataset\n",
    "from src.models.variational_layers.variational_layer import HorseshoeLayer_out_mask\n",
    "from src.models.variational_layers.linear_reparam import LinearReparam\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from src.data_prep.load_data import load_cpath_data\n",
    "from src.models.loss_functions.loss_functions import partial_ll_loss\n",
    "\n",
    "from numpy.random import normal\n",
    "from numpy import sin\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "''' Net Settings'''\n",
    "In_Nodes = 5567 ###number of genes\n",
    "Pathway_Nodes = 860 ###number of pathways\n",
    "Hidden_Nodes = 100 ###number of hidden nodes\n",
    "Out_Nodes = 30 ###number of hidden nodes in the last hidden layer\n",
    "''' Initialize '''\n",
    "Initial_Learning_Rate = [0.03] #[0.03, 0.01, 0.001, 0.00075]\n",
    "L2_Lambda = [0.01]  #[0.1, 0.01, 0.005, 0.001]\n",
    "num_epochs = 10 #3000 ###for grid search\n",
    "Num_EPOCHS = 15 #20000 ###for training\n",
    "###sub-network setup\n",
    "Dropout_Rate = [0.7,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "''' load data and pathway '''\n",
    "pathway_mask = load_pathway(\"../data/pathway_mask.csv\", dtype)\n",
    "\n",
    "x_train, ytime_train, yevent_train, age_train = load_data(\"../data/train.csv\", dtype)\n",
    "x_valid, ytime_valid, yevent_valid, age_valid = load_data(\"../data/validation.csv\", dtype)\n",
    "x_test, ytime_test, yevent_test, age_test = load_data(\"../data/test.csv\", dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Horseshoe Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = normal(0,1,2000)\n",
    "x2 = normal(0,1,2000)\n",
    "x3 = normal(0,1,2000)\n",
    "x4 = normal(0,1,2000)\n",
    "features  = np.array([x1,x2,x3,x4])\n",
    "features = np.transpose(features)\n",
    "y = x1 + x2\n",
    "\n",
    "x_df = pd.DataFrame(features, columns=['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4'])\n",
    "y_df = pd.DataFrame(y, columns=[\"label\"])\n",
    "\n",
    "x_tens = torch.tensor(features)\n",
    "y_tens = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Horseshoe_params:\n",
    "    def __init__(self):\n",
    "        self.horseshoe_scale = None\n",
    "        self.global_cauchy_scale = 2.4\n",
    "        self.weight_cauchy_scale = 15\n",
    "        self.beta_rho_scale = -5.\n",
    "        self.log_tau_mean = None\n",
    "        self.log_tau_rho_scale = -5.\n",
    "        self.bias_rho_scale = -5.\n",
    "        self.log_v_mean = None\n",
    "        self.log_v_rho_scale = -5.\n",
    "\n",
    "hs_parameters = Horseshoe_params()\n",
    "\n",
    "class hsreg(nn.Module):\n",
    "    def __init__(self, In_Nodes, Pathway_Nodes,Hidden_Nodes,mask):\n",
    "        super(hsreg, self).__init__()\n",
    "        # activation\n",
    "        self.tanh = nn.Tanh()\n",
    "        # layers\n",
    "        self.fc1 = HorseshoeLayer_out_mask(In_Nodes, Pathway_Nodes, hs_parameters, mask=mask)\n",
    "        self.fc2 = LinearReparam(in_features=Pathway_Nodes,\n",
    "                                out_features=Hidden_Nodes,\n",
    "                                prior_means=np.full((Hidden_Nodes, Pathway_Nodes), 0),\n",
    "                                prior_variances=np.full((Hidden_Nodes, Pathway_Nodes), 0.2),\n",
    "                                posterior_mu_init=np.full((Hidden_Nodes, Pathway_Nodes), 0.5),\n",
    "                                posterior_rho_init=np.full((Hidden_Nodes, Pathway_Nodes), -3.),\n",
    "                                bias=False,\n",
    "                                )\n",
    "\n",
    "        # layers including kl_divergence\n",
    "        self.kl_list = [self.fc1,self.fc2]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.fc2(x.squeeze(0),return_kl = False)\n",
    "       \n",
    "\n",
    "        return x\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        KLD = 0\n",
    "        for layer in self.kl_list:\n",
    "            KLD += layer.kl_divergence()\n",
    "        return KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_mask = torch.ones(2,4)\n",
    "#i_mask[1][0] = 0 \n",
    "#i_mask[1][1] = 0 \n",
    "#i_mask[0][2] = 0 \n",
    "#i_mask[0][3] = 0 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hsreg(4, 2, 1, mask=i_mask)\n",
    "model.double()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "for epoch in range(1, 500):\n",
    "                # measure data loading time\n",
    "         \n",
    "                output_ = []\n",
    "                for mc_run in range(200):\n",
    "                    output = model(x_tens.double())\n",
    "                    output_.append(output)\n",
    "                output = torch.mean(torch.stack(output_), dim=0)\n",
    "                loss_crit_metric = loss(output[1],y_tens)\n",
    "                scaled_kl = model.kl_divergence()  # should these things be batchsize / dataset?\n",
    "                total_loss = loss_crit_metric + scaled_kl\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                model.fc1.analytic_update()\n",
    "                print(total_loss)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1912, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y_pred,y_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_mean Parameter containing:\n",
      "tensor([[-0.0062, -0.0049, -0.1367, -0.0019],\n",
      "        [-0.0024, -0.0638, -0.1001,  0.0562]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "beta_rho Parameter containing:\n",
      "tensor([[-4.0109, -4.0109, -4.0109, -4.0109],\n",
      "        [-4.0109, -4.0109, -4.0109, -4.0109]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "log_tau_mean Parameter containing:\n",
      "tensor([3.4030, 2.0818], dtype=torch.float64, requires_grad=True)\n",
      "log_tau_rho Parameter containing:\n",
      "tensor([-4.0109, -4.0109], dtype=torch.float64, requires_grad=True)\n",
      "bias_mean Parameter containing:\n",
      "tensor([[-5.2339e-05, -7.0357e-04]], dtype=torch.float64, requires_grad=True)\n",
      "bias_rho Parameter containing:\n",
      "tensor([[-4.0109, -4.0109]], dtype=torch.float64, requires_grad=True)\n",
      "log_v_mean Parameter containing:\n",
      "tensor([1.7385], dtype=torch.float64, requires_grad=True)\n",
      "log_v_rho Parameter containing:\n",
      "tensor([[-4.0109]], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.fc1.named_parameters():\n",
    "    print (name,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.2566e-01, -9.0565e-01,  1.7676e+00, -2.9764e-01,  1.2536e+00,\n",
       "         1.4750e+00,  1.5163e+00, -1.0396e+00, -6.7744e-01, -8.2130e-02,\n",
       "         1.2070e+00, -3.3554e-01, -8.1102e-01, -1.7456e+00,  9.4410e-01,\n",
       "         2.9364e-01, -4.2814e-01, -7.7999e-01,  1.0572e+00,  1.8253e-01,\n",
       "        -6.4868e-01, -1.1367e-01,  8.8010e-01,  1.7157e+00,  2.0295e-01,\n",
       "         9.9686e-01,  8.2852e-01, -1.5172e+00,  6.9478e-01,  2.7035e-01,\n",
       "         1.6998e-02,  1.0618e+00,  9.3060e-02,  1.4571e+00,  1.7141e-02,\n",
       "         1.6347e+00, -1.8463e+00,  1.9802e+00, -5.9827e-01, -1.4648e+00,\n",
       "         6.7629e-01, -2.8161e-01, -1.3147e+00, -3.1017e-01,  1.7263e+00,\n",
       "        -1.6633e+00, -1.9120e+00,  1.2116e+00,  1.6744e-01, -5.0710e-01,\n",
       "        -7.0748e-01,  2.5065e-01, -7.1036e-01, -1.1587e+00,  1.1562e+00,\n",
       "        -3.0361e-01,  1.5003e+00,  1.4350e+00, -3.6483e-01,  1.1510e+00,\n",
       "        -3.7250e-01, -9.3640e-01,  7.0712e-01,  6.1646e-01,  9.3286e-01,\n",
       "        -4.8368e-01, -1.4867e-02,  7.2249e-01,  4.7145e-01, -4.9555e-01,\n",
       "        -1.9424e+00,  2.0289e-01,  4.8523e-01, -4.9003e-01,  1.4479e-02,\n",
       "         7.7214e-01, -8.7129e-01, -9.5899e-01,  2.4083e-01, -2.5489e-02,\n",
       "        -1.4090e-01,  1.9144e+00, -1.7482e+00,  9.0409e-02,  1.6063e+00,\n",
       "         9.8988e-01, -4.8096e-01, -1.1452e+00,  3.1470e-01,  6.2801e-01,\n",
       "        -1.5273e+00,  1.1268e+00,  7.3020e-01, -1.3335e+00, -1.3447e+00,\n",
       "        -6.7187e-01,  6.4936e-01, -1.1927e+00,  3.3292e-01, -4.7415e-01,\n",
       "         3.6918e-01, -2.7962e-01,  6.7459e-01,  7.7937e-01, -8.9848e-01,\n",
       "         1.7801e+00, -1.0362e+00,  1.2554e-01, -9.4185e-01,  4.7859e-01,\n",
       "         7.0505e-01, -1.0619e+00, -1.7311e+00,  8.2477e-01,  5.6953e-01,\n",
       "         1.3885e+00,  6.6887e-01,  9.8976e-02, -1.1464e+00, -4.4175e-01,\n",
       "         1.7075e+00, -2.4553e-01,  9.1457e-02,  1.7702e-02, -3.9073e-02,\n",
       "        -6.8238e-01,  3.5273e-01,  5.3543e-02, -3.6525e-02, -4.4370e-01,\n",
       "         4.6684e-01,  2.2349e-03, -7.1058e-01, -9.4587e-01,  1.8414e+00,\n",
       "         2.0174e-01,  1.2654e-01, -6.0088e-02,  8.5945e-02,  1.1635e+00,\n",
       "        -1.3892e+00,  6.2621e-01, -9.5015e-02,  7.1307e-01,  1.6928e-01,\n",
       "        -2.0712e-01, -8.2424e-02, -1.2631e+00, -1.2633e+00, -6.4730e-01,\n",
       "        -3.1975e-01,  2.2358e-01, -9.0451e-02, -1.6403e+00,  2.4855e-01,\n",
       "         1.2713e+00,  1.0650e+00,  4.4180e-01,  3.7934e-01,  1.4236e+00,\n",
       "         1.0411e+00, -7.9081e-01,  1.0932e+00, -6.6627e-01, -1.5319e+00,\n",
       "        -4.9602e-01, -3.5317e-01,  1.0545e-02, -1.4249e+00,  1.7702e-01,\n",
       "        -6.6577e-01,  4.9119e-01,  1.4742e+00, -7.6176e-02, -4.3121e-02,\n",
       "        -5.1915e-01, -1.4840e+00, -3.7783e-01,  1.1574e+00,  1.3410e+00,\n",
       "        -7.2957e-02, -7.3303e-01,  3.0283e-01,  5.1140e-01, -7.1672e-01,\n",
       "        -1.5804e+00,  5.8109e-01,  1.5145e+00,  2.9880e-01, -8.2505e-01,\n",
       "        -8.1041e-01, -6.9462e-01, -6.5042e-02,  1.9883e-01,  1.5487e+00,\n",
       "         7.8426e-02,  1.2811e+00, -9.7922e-01,  7.4874e-01, -1.4723e+00,\n",
       "         5.1974e-01, -1.9480e+00,  2.5191e-01,  1.2619e+00, -1.8723e+00,\n",
       "         1.3379e+00,  1.6777e+00, -1.5302e+00, -2.9053e-01, -1.8193e+00,\n",
       "         1.5872e+00, -7.0707e-01, -1.4068e-02,  1.0347e+00,  9.1995e-01,\n",
       "        -3.1224e-01, -4.1569e-01, -3.1980e-01, -1.4075e+00,  5.0056e-01,\n",
       "        -1.1252e+00,  1.2157e+00, -5.0311e-01, -1.9205e+00, -7.5026e-01,\n",
       "        -9.0411e-01,  1.6953e+00, -3.8293e-01, -3.8222e-01,  1.5140e+00,\n",
       "         1.9269e+00,  1.4933e+00,  8.8758e-02,  9.7770e-01,  5.7435e-01,\n",
       "        -1.2274e+00,  8.7757e-01, -3.3760e-01,  4.3868e-01,  6.7947e-01,\n",
       "         9.6275e-01,  1.7479e+00, -4.9355e-01,  1.1243e+00, -8.7173e-01,\n",
       "         1.0402e-01,  3.6479e-01, -3.5172e-02, -4.9240e-01, -1.9857e-01,\n",
       "         1.7218e+00, -6.4839e-02,  1.5718e-01, -1.3942e+00,  9.7646e-02,\n",
       "         1.7758e-01, -3.7441e-01, -1.8651e+00, -1.6883e+00, -3.4769e-01,\n",
       "         1.0530e+00,  1.0304e+00,  1.0895e+00, -1.6209e-01,  6.4208e-01,\n",
       "        -3.4415e-01, -6.6863e-01,  7.4412e-01, -1.9511e-01,  9.4226e-01,\n",
       "        -6.2141e-01, -5.8649e-01,  1.3953e+00,  5.5253e-01,  8.8361e-01,\n",
       "        -9.0424e-01, -8.0279e-01,  1.6029e-01, -3.9143e-01,  4.2399e-01,\n",
       "         2.5765e-01, -6.1819e-01,  1.8138e+00,  1.0001e-01, -6.8768e-01,\n",
       "        -5.5216e-01, -5.4248e-01,  2.7635e-04,  1.3188e-01,  1.0777e+00,\n",
       "        -1.7431e+00,  2.4832e-01,  1.8348e+00, -2.1598e-01, -8.1148e-01,\n",
       "        -3.5872e-01,  6.7298e-01, -2.9603e-01, -7.9247e-01, -1.2243e+00,\n",
       "        -7.2761e-03,  2.1817e-02, -1.8093e+00,  1.2907e-01,  1.0601e+00,\n",
       "         4.8565e-02,  1.4940e-03,  4.1223e-01,  6.2169e-02,  6.7889e-01,\n",
       "        -1.0896e+00,  4.9571e-03,  1.2043e+00,  1.5898e+00,  1.3069e-01,\n",
       "        -1.2077e+00,  1.5391e+00,  1.5494e-01, -1.2531e+00,  1.0639e+00,\n",
       "         1.6318e+00, -8.9234e-01, -6.2606e-01, -9.7972e-01,  1.2577e-02,\n",
       "        -4.0498e-02, -7.4365e-01, -9.3191e-01, -1.4007e+00, -3.0122e-01,\n",
       "         2.9668e-01,  9.1205e-01,  1.0335e+00,  1.5573e+00,  1.9224e+00,\n",
       "         9.8921e-03, -6.0847e-01, -5.9910e-01,  1.7087e+00, -6.0591e-02,\n",
       "         1.2765e+00,  9.2879e-01, -3.0164e-01,  5.2861e-01,  1.0022e+00,\n",
       "        -4.1133e-01, -4.0862e-01, -6.3531e-01, -1.4629e+00, -1.4679e-01,\n",
       "        -9.6807e-01,  1.3348e+00,  1.8798e-01,  5.6613e-02, -7.6338e-01,\n",
       "        -1.5556e-01,  1.1229e+00, -8.5670e-01,  6.1168e-01,  6.8793e-01,\n",
       "         7.0437e-02, -1.8738e-01, -1.0736e+00, -3.1694e-01, -7.1246e-01,\n",
       "         1.0382e+00,  4.4577e-01, -7.9970e-01, -1.3786e+00,  6.9951e-01,\n",
       "        -7.5567e-01,  5.0414e-01,  1.4656e+00,  3.8211e-01, -1.8667e+00,\n",
       "        -9.4415e-01,  9.8653e-01, -5.6753e-01,  1.6105e+00, -5.6117e-01,\n",
       "         1.1755e+00,  1.1809e+00, -1.3533e+00, -3.5816e-01, -1.3751e+00,\n",
       "        -6.0499e-01,  1.4786e+00, -2.3370e-02, -7.3904e-01, -1.7609e+00,\n",
       "         6.7866e-01, -4.1967e-01,  3.4450e-01, -1.7586e+00, -7.4966e-01,\n",
       "        -6.4414e-01, -6.4802e-01, -8.9792e-01,  1.0768e+00,  6.8836e-01,\n",
       "         1.2442e+00,  1.2645e+00,  1.1410e+00,  1.4090e-01,  7.9348e-01,\n",
       "        -1.6077e+00,  1.8294e+00,  4.3891e-02,  1.1300e+00,  8.9583e-01,\n",
       "         4.6744e-02, -1.9031e-01,  1.7437e+00, -1.6414e+00, -1.9444e+00,\n",
       "        -1.2893e-01,  8.4230e-01,  2.3908e-01, -1.2776e+00,  5.5464e-01,\n",
       "         1.7998e+00, -6.0132e-01,  1.4756e+00, -5.6639e-01, -1.4178e-01,\n",
       "        -5.7192e-01, -2.1321e-01, -1.2682e+00, -1.8853e+00,  1.3031e+00,\n",
       "        -1.4112e+00, -5.5839e-01,  7.4539e-01,  9.8786e-01, -7.0172e-01,\n",
       "         1.3064e+00,  6.8579e-01,  1.9833e-01, -7.5825e-01,  7.7977e-01,\n",
       "         5.4002e-02,  1.5224e+00, -6.2639e-01, -1.4322e+00, -5.5053e-01,\n",
       "         2.1953e-01,  1.0376e-01,  1.5177e-02,  4.1132e-01, -4.4045e-01,\n",
       "         7.7257e-01,  6.1421e-01,  1.0249e+00,  1.6577e-01, -5.6591e-01,\n",
       "         1.3350e+00, -1.2690e+00,  4.4477e-01, -2.1994e-01,  1.7651e+00,\n",
       "         2.5167e-01, -3.8815e-01, -5.9946e-01, -1.3923e+00, -1.5150e+00,\n",
       "         1.5878e+00,  1.2212e+00,  3.9926e-01, -1.5499e+00, -2.7394e-01,\n",
       "        -1.4282e+00, -2.1446e-01,  2.3910e-01,  1.0635e-01, -1.0230e-01,\n",
       "        -6.2693e-02,  8.2362e-03, -2.4976e-01, -6.3575e-02, -1.2132e+00,\n",
       "         2.5279e-02, -9.5741e-02,  4.0831e-01,  7.3466e-02,  1.7214e+00,\n",
       "        -7.2716e-01,  4.6106e-01,  1.0931e-01,  6.2382e-01,  6.5760e-01,\n",
       "         4.3344e-01,  1.8315e+00, -8.4698e-01, -4.4528e-01,  4.2838e-01,\n",
       "        -1.7688e+00, -7.8094e-01,  2.4453e-01,  1.4100e+00, -1.7832e+00],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpath_train_loader,cpath_test_loader,cpath_val_loader,pathway_mask = load_cpath_data(cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Horseshoe_params:\n",
    "    def __init__(self):\n",
    "        self.horseshoe_scale = None\n",
    "        self.global_cauchy_scale = 1.\n",
    "        self.weight_cauchy_scale = 1.\n",
    "        self.beta_rho_scale = -5.\n",
    "        self.log_tau_mean = None\n",
    "        self.log_tau_rho_scale = -5.\n",
    "        self.bias_rho_scale = -5.\n",
    "        self.log_v_mean = None\n",
    "        self.log_v_rho_scale = -5.\n",
    "\n",
    "hs_parameters = Horseshoe_params()\n",
    "\n",
    "class hs_nn(nn.Module):\n",
    "    def __init__(self, In_Nodes, Pathway_Nodes,Hidden_Nodes,mask):\n",
    "        super(hs_nn, self).__init__()\n",
    "        # activation\n",
    "        self.tanh = nn.Tanh()\n",
    "        # layers\n",
    "        self.fc1 = HorseshoeLayer_out_mask(In_Nodes, Pathway_Nodes, hs_parameters, mask=mask)\n",
    "        self.fc2 = LinearReparam(in_features=Pathway_Nodes,\n",
    "                                out_features=Hidden_Nodes,\n",
    "                                prior_means=np.full((Hidden_Nodes, Pathway_Nodes), 0),\n",
    "                                prior_variances=np.full((Hidden_Nodes, Pathway_Nodes), 0.2),\n",
    "                                posterior_mu_init=np.full((Hidden_Nodes, Pathway_Nodes), 0.5),\n",
    "                                posterior_rho_init=np.full((Hidden_Nodes, Pathway_Nodes), -3.),\n",
    "                                bias=False,\n",
    "                                )\n",
    "\n",
    "        # layers including kl_divergence\n",
    "        self.kl_list = [self.fc1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.fc2(x.squeeze(0),return_kl = False)\n",
    "       \n",
    "\n",
    "        return x\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        KLD = 0\n",
    "        for layer in self.kl_list:\n",
    "            KLD += layer.kl_divergence()\n",
    "        return KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([860, 5567])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.beta_mean\n",
      "fc1.beta_rho\n",
      "fc1.log_tau_mean\n",
      "fc1.log_tau_rho\n",
      "fc1.bias_mean\n",
      "fc1.bias_rho\n",
      "fc1.log_v_mean\n",
      "fc1.log_v_rho\n",
      "fc2.mu_weight\n",
      "fc2.rho_weight\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = hs_nn(5567, 860, 1, mask=pathway_mask)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4754662282030202\n",
      "0.4870624378001817\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(1, 3):\n",
    "    for i, (input, target) in enumerate(cpath_train_loader):\n",
    "                # measure data loading time\n",
    "\n",
    "\n",
    "                tb = target[\"tb\"].cpu()\n",
    "                e = target[\"e\"].cpu()\n",
    "                input_var = input[\"X\"].cpu()\n",
    "                clinical_var = input[\"clinical_vars\"].cpu()\n",
    "\n",
    "                output_ = []\n",
    "                for mc_run in range(100):\n",
    "                    output = model(input_var)\n",
    "                    output_.append(output)\n",
    "                output = torch.mean(torch.stack(output_), dim=0)\n",
    "                loss_crit_metric = partial_ll_loss(output.reshape(-1).cpu(), tb.reshape(-1).cpu(), e.reshape(-1).cpu())\n",
    "                scaled_loss_crit_metric = loss_crit_metric * (len(cpath_train_loader.dataset) / cpath_train_loader.batch_size)  # this might be the other way round\n",
    "                scaled_kl = model.kl_divergence() / cpath_train_loader.batch_size  # should these things be batchsize / dataset?\n",
    "                #loss = scaled_loss_crit_metric + scaled_kl\n",
    "                loss = scaled_kl\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                model.fc1.analytic_update()\n",
    "\n",
    "                conc_metric = concordance_index_censored(e.detach().cpu().numpy().astype(bool).reshape(-1),\n",
    "                                                         tb.detach().cpu().numpy().reshape(-1),\n",
    "                                                         output.reshape(-1).detach().cpu().numpy())[0]\n",
    "                print(conc_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2190, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Group Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearGroupNJ_Masked(Module):\n",
    "    \"\"\"Fully Connected Group Normal-Jeffrey's layer (aka Group Variational Dropout).\n",
    "    References:\n",
    "    [1] Kingma, Diederik P., Tim Salimans, and Max Welling. \"Variational dropout and the local reparameterization trick.\" NIPS (2015).\n",
    "    [2] Molchanov, Dmitry, Arsenii Ashukha, and Dmitry Vetrov. \"Variational Dropout Sparsifies Deep Neural Networks.\" ICML (2017).\n",
    "    [3] Louizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian Compression for Deep Learning.\" NIPS (2017).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, cuda=False, init_weight=None, init_bias=None, clip_var=None):\n",
    "\n",
    "        super(LinearGroupNJ, self).__init__()\n",
    "        self.cuda = cuda\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.clip_var = clip_var\n",
    "        self.deterministic = False  # flag is used for compressed inference\n",
    "        # trainable params according to Eq.(6)\n",
    "        # dropout params\n",
    "        self.z_mu = Parameter(torch.Tensor(in_features))\n",
    "        self.z_logvar = Parameter(torch.Tensor(in_features))  # = z_mu^2 * alpha\n",
    "        # weight params\n",
    "        self.weight_mu = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_logvar = Parameter(torch.Tensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = Parameter(torch.Tensor(out_features))\n",
    "        self.bias_logvar = Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        # init params either random or with pretrained net\n",
    "        self.reset_parameters(init_weight, init_bias)\n",
    "\n",
    "        # activations for kl\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        # numerical stability param\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def reset_parameters(self, init_weight, init_bias):\n",
    "        # init means\n",
    "        stdv = 1. / math.sqrt(self.weight_mu.size(1))\n",
    "\n",
    "        self.z_mu.data.normal_(1, 1e-2)\n",
    "\n",
    "        if init_weight is not None:\n",
    "            self.weight_mu.data = self.mask * torch.Tensor(init_weight)\n",
    "        else:\n",
    "            self.weight_mu.data.normal_(0, stdv)\n",
    "            self.weight_mu.data = self.mask * self.weight_mu \n",
    "\n",
    "        if init_bias is not None:\n",
    "            self.bias_mu.data = torch.Tensor(init_bias)\n",
    "        else:\n",
    "            self.bias_mu.data.fill_(0)\n",
    "\n",
    "        # init logvars\n",
    "        self.z_logvar.data.normal_(-9, 1e-2)\n",
    "        self.weight_logvar.data.normal_(-9, 1e-2)\n",
    "        self.bias_logvar.data.normal_(-9, 1e-2)\n",
    "\n",
    "    def clip_variances(self):\n",
    "        if self.clip_var:\n",
    "            self.weight_logvar.data.clamp_(max=math.log(self.clip_var))\n",
    "            self.bias_logvar.data.clamp_(max=math.log(self.clip_var))\n",
    "\n",
    "    def get_log_dropout_rates(self):\n",
    "        log_alpha = self.z_logvar - torch.log(self.z_mu.pow(2) + self.epsilon)\n",
    "        return log_alpha\n",
    "\n",
    "    def compute_posterior_params(self):\n",
    "        weight_var, z_var = self.weight_logvar.exp(), self.z_logvar.exp()\n",
    "        self.post_weight_var = self.z_mu.pow(2) * weight_var + z_var * self.weight_mu.pow(2) + z_var * weight_var\n",
    "        self.post_weight_mu = self.weight_mu * self.z_mu\n",
    "        return self.post_weight_mu, self.post_weight_var\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.deterministic:\n",
    "            assert self.training == False, \"Flag deterministic is True. This should not be used in training.\"\n",
    "            return F.linear(x, self.post_weight_mu, self.bias_mu)\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "        # compute z  \n",
    "        # note that we reparametrise according to [2] Eq. (11) (not [1])\n",
    "        z = reparametrize(self.z_mu.repeat(batch_size, 1), self.z_logvar.repeat(batch_size, 1), sampling=self.training,\n",
    "                          cuda=self.cuda)\n",
    "\n",
    "        # apply local reparametrisation trick see [1] Eq. (6)\n",
    "        # to the parametrisation given in [3] Eq. (6)\n",
    "        xz = x * z\n",
    "        mu_activations = F.linear(xz, self.weight_mu, self.bias_mu)\n",
    "        var_activations = F.linear(xz.pow(2), self.weight_logvar.exp(), self.bias_logvar.exp())\n",
    "\n",
    "        return reparametrize(mu_activations, var_activations.log(), sampling=self.training, cuda=self.cuda)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        # KL(q(z)||p(z))\n",
    "        # we use the kl divergence approximation given by [2] Eq.(14)\n",
    "        k1, k2, k3 = 0.63576, 1.87320, 1.48695\n",
    "        log_alpha = self.get_log_dropout_rates()\n",
    "        KLD = -torch.sum(k1 * self.sigmoid(k2 + k3 * log_alpha) - 0.5 * self.softplus(-log_alpha) - k1)\n",
    "\n",
    "        # KL(q(w|z)||p(w|z))\n",
    "        # we use the kl divergence given by [3] Eq.(8)\n",
    "        KLD_element = -0.5 * self.weight_logvar + 0.5 * (self.weight_logvar.exp() + self.weight_mu.pow(2)) - 0.5\n",
    "        KLD += torch.sum(KLD_element)\n",
    "\n",
    "        # KL bias\n",
    "        KLD_element = -0.5 * self.bias_logvar + 0.5 * (self.bias_logvar.exp() + self.bias_mu.pow(2)) - 0.5\n",
    "        KLD += torch.sum(KLD_element)\n",
    "\n",
    "        return KLD\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3019,  0.3189, -0.2703,  ..., -0.0904, -0.2369, -0.0382],\n",
       "        [-0.1145,  0.3655,  0.1703,  ..., -0.1354,  0.2717, -0.0121],\n",
       "        [-0.1308,  0.1574,  0.1233,  ...,  0.0948, -0.1653,  0.0116],\n",
       "        ...,\n",
       "        [ 0.2184,  0.3030,  0.0014,  ..., -0.0902,  0.0674,  0.0728],\n",
       "        [-0.2955,  0.2499, -0.2631,  ...,  0.2403,  0.0836, -0.0193],\n",
       "        [-0.1089, -0.2964,  0.0898,  ..., -0.1847,  0.1355,  0.1329]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tens = Parameter(torch.Tensor(860, 5567))\n",
    "test_tens.data.normal_(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([860, 5567])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (860) must match the size of tensor b (5567) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/dy/sjxgt5r91_s5m71lvl0gghh80000gn/T/ipykernel_22128/1409726567.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpathway_mask\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mtest_tens\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (860) must match the size of tensor b (5567) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = pathway_mask.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (860) must match the size of tensor b (5567) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/dy/sjxgt5r91_s5m71lvl0gghh80000gn/T/ipykernel_22128/594718580.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0ml\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mtest_tens\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (860) must match the size of tensor b (5567) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "l*test_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pathway_mask *test_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5567, 860])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([860, 5567])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}