{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from CoxPASNet.coxpasnet.DataLoader import load_data, load_pathway\n",
    "from CoxPASNet.coxpasnet.Train import trainCoxPASNet\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from CoxPASNet.coxpasnet.Survival_CostFunc_CIndex import R_set, neg_par_log_likelihood, c_index\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from bayesian_torch.models.dnn_to_bnn import dnn_to_bnn, get_kl_loss\n",
    "\n",
    "from run_pipeline.bayesian_custom_nn import run_custom_bnn,arguments\n",
    "from run_pipeline.cpath_bnn import run_cpath_bnn,arguments\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from src.models.variational_layers.linear_reparam import LinearReparam,LinearGroupNJ\n",
    "from src.data_prep.torch_datasets import cpath_dataset\n",
    "from torch.nn.parameter import Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "''' Net Settings'''\n",
    "In_Nodes = 5567 ###number of genes\n",
    "Pathway_Nodes = 860 ###number of pathways\n",
    "Hidden_Nodes = 100 ###number of hidden nodes\n",
    "Out_Nodes = 30 ###number of hidden nodes in the last hidden layer\n",
    "''' Initialize '''\n",
    "Initial_Learning_Rate = [0.03] #[0.03, 0.01, 0.001, 0.00075]\n",
    "L2_Lambda = [0.01]  #[0.1, 0.01, 0.005, 0.001]\n",
    "num_epochs = 10 #3000 ###for grid search\n",
    "Num_EPOCHS = 15 #20000 ###for training\n",
    "###sub-network setup\n",
    "Dropout_Rate = [0.7,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "''' load data and pathway '''\n",
    "pathway_mask = load_pathway(\"../data/pathway_mask.csv\", dtype)\n",
    "\n",
    "x_train, ytime_train, yevent_train, age_train = load_data(\"../data/train.csv\", dtype)\n",
    "x_valid, ytime_valid, yevent_valid, age_valid = load_data(\"../data/validation.csv\", dtype)\n",
    "x_test, ytime_test, yevent_test, age_test = load_data(\"../data/test.csv\", dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearGroupNJ_Masked(Module):\n",
    "    \"\"\"Fully Connected Group Normal-Jeffrey's layer (aka Group Variational Dropout).\n",
    "    References:\n",
    "    [1] Kingma, Diederik P., Tim Salimans, and Max Welling. \"Variational dropout and the local reparameterization trick.\" NIPS (2015).\n",
    "    [2] Molchanov, Dmitry, Arsenii Ashukha, and Dmitry Vetrov. \"Variational Dropout Sparsifies Deep Neural Networks.\" ICML (2017).\n",
    "    [3] Louizos, Christos, Karen Ullrich, and Max Welling. \"Bayesian Compression for Deep Learning.\" NIPS (2017).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, cuda=False, init_weight=None, init_bias=None, clip_var=None):\n",
    "\n",
    "        super(LinearGroupNJ, self).__init__()\n",
    "        self.cuda = cuda\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.clip_var = clip_var\n",
    "        self.deterministic = False  # flag is used for compressed inference\n",
    "        # trainable params according to Eq.(6)\n",
    "        # dropout params\n",
    "        self.z_mu = Parameter(torch.Tensor(in_features))\n",
    "        self.z_logvar = Parameter(torch.Tensor(in_features))  # = z_mu^2 * alpha\n",
    "        # weight params\n",
    "        self.weight_mu = Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_logvar = Parameter(torch.Tensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = Parameter(torch.Tensor(out_features))\n",
    "        self.bias_logvar = Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        # init params either random or with pretrained net\n",
    "        self.reset_parameters(init_weight, init_bias)\n",
    "\n",
    "        # activations for kl\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        # numerical stability param\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def reset_parameters(self, init_weight, init_bias):\n",
    "        # init means\n",
    "        stdv = 1. / math.sqrt(self.weight_mu.size(1))\n",
    "\n",
    "        self.z_mu.data.normal_(1, 1e-2)\n",
    "\n",
    "        if init_weight is not None:\n",
    "            self.weight_mu.data = self.mask * torch.Tensor(init_weight)\n",
    "        else:\n",
    "            self.weight_mu.data.normal_(0, stdv)\n",
    "            self.weight_mu.data = self.mask * self.weight_mu \n",
    "\n",
    "        if init_bias is not None:\n",
    "            self.bias_mu.data = torch.Tensor(init_bias)\n",
    "        else:\n",
    "            self.bias_mu.data.fill_(0)\n",
    "\n",
    "        # init logvars\n",
    "        self.z_logvar.data.normal_(-9, 1e-2)\n",
    "        self.weight_logvar.data.normal_(-9, 1e-2)\n",
    "        self.bias_logvar.data.normal_(-9, 1e-2)\n",
    "\n",
    "    def clip_variances(self):\n",
    "        if self.clip_var:\n",
    "            self.weight_logvar.data.clamp_(max=math.log(self.clip_var))\n",
    "            self.bias_logvar.data.clamp_(max=math.log(self.clip_var))\n",
    "\n",
    "    def get_log_dropout_rates(self):\n",
    "        log_alpha = self.z_logvar - torch.log(self.z_mu.pow(2) + self.epsilon)\n",
    "        return log_alpha\n",
    "\n",
    "    def compute_posterior_params(self):\n",
    "        weight_var, z_var = self.weight_logvar.exp(), self.z_logvar.exp()\n",
    "        self.post_weight_var = self.z_mu.pow(2) * weight_var + z_var * self.weight_mu.pow(2) + z_var * weight_var\n",
    "        self.post_weight_mu = self.weight_mu * self.z_mu\n",
    "        return self.post_weight_mu, self.post_weight_var\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.deterministic:\n",
    "            assert self.training == False, \"Flag deterministic is True. This should not be used in training.\"\n",
    "            return F.linear(x, self.post_weight_mu, self.bias_mu)\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "        # compute z  \n",
    "        # note that we reparametrise according to [2] Eq. (11) (not [1])\n",
    "        z = reparametrize(self.z_mu.repeat(batch_size, 1), self.z_logvar.repeat(batch_size, 1), sampling=self.training,\n",
    "                          cuda=self.cuda)\n",
    "\n",
    "        # apply local reparametrisation trick see [1] Eq. (6)\n",
    "        # to the parametrisation given in [3] Eq. (6)\n",
    "        xz = x * z\n",
    "        mu_activations = F.linear(xz, self.weight_mu, self.bias_mu)\n",
    "        var_activations = F.linear(xz.pow(2), self.weight_logvar.exp(), self.bias_logvar.exp())\n",
    "\n",
    "        return reparametrize(mu_activations, var_activations.log(), sampling=self.training, cuda=self.cuda)\n",
    "\n",
    "    def kl_divergence(self):\n",
    "        # KL(q(z)||p(z))\n",
    "        # we use the kl divergence approximation given by [2] Eq.(14)\n",
    "        k1, k2, k3 = 0.63576, 1.87320, 1.48695\n",
    "        log_alpha = self.get_log_dropout_rates()\n",
    "        KLD = -torch.sum(k1 * self.sigmoid(k2 + k3 * log_alpha) - 0.5 * self.softplus(-log_alpha) - k1)\n",
    "\n",
    "        # KL(q(w|z)||p(w|z))\n",
    "        # we use the kl divergence given by [3] Eq.(8)\n",
    "        KLD_element = -0.5 * self.weight_logvar + 0.5 * (self.weight_logvar.exp() + self.weight_mu.pow(2)) - 0.5\n",
    "        KLD += torch.sum(KLD_element)\n",
    "\n",
    "        # KL bias\n",
    "        KLD_element = -0.5 * self.bias_logvar + 0.5 * (self.bias_logvar.exp() + self.bias_mu.pow(2)) - 0.5\n",
    "        KLD += torch.sum(KLD_element)\n",
    "\n",
    "        return KLD\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3019,  0.3189, -0.2703,  ..., -0.0904, -0.2369, -0.0382],\n",
       "        [-0.1145,  0.3655,  0.1703,  ..., -0.1354,  0.2717, -0.0121],\n",
       "        [-0.1308,  0.1574,  0.1233,  ...,  0.0948, -0.1653,  0.0116],\n",
       "        ...,\n",
       "        [ 0.2184,  0.3030,  0.0014,  ..., -0.0902,  0.0674,  0.0728],\n",
       "        [-0.2955,  0.2499, -0.2631,  ...,  0.2403,  0.0836, -0.0193],\n",
       "        [-0.1089, -0.2964,  0.0898,  ..., -0.1847,  0.1355,  0.1329]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tens = Parameter(torch.Tensor(860, 5567))\n",
    "test_tens.data.normal_(0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([860, 5567])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (860) must match the size of tensor b (5567) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/dy/sjxgt5r91_s5m71lvl0gghh80000gn/T/ipykernel_22128/1409726567.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpathway_mask\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mtest_tens\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (860) must match the size of tensor b (5567) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = pathway_mask.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (860) must match the size of tensor b (5567) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/dy/sjxgt5r91_s5m71lvl0gghh80000gn/T/ipykernel_22128/594718580.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0ml\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mtest_tens\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (860) must match the size of tensor b (5567) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "l*test_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pathway_mask *test_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5567, 860])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([860, 5567])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}