{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyg\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "#from torch_geometric.loader import DataLoader\n",
    "import torch_geometric\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathcnn_notebook_utils import data_prep_pathcnn_paper,create_overlap_adjacency,prepare_vanilla_loader,prepare_cnn_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_exp = pd.read_excel(\"/Users/alexandermollers/Documents/GitHub/survival_analysis/data/multiomics/PCA_EXP.xlsx\", header=None)\n",
    "pca_cnv = pd.read_excel(\"/Users/alexandermollers/Documents/GitHub/survival_analysis/data/multiomics/PCA_CNV.xlsx\", header=None)\n",
    "pca_mt = pd.read_excel(\"/Users/alexandermollers/Documents/GitHub/survival_analysis/data/multiomics/PCA_MT.xlsx\", header=None)\n",
    "ordered_pathways = pd.read_excel(\"/Users/alexandermollers/Documents/GitHub/survival_analysis/data/multiomics/ordered_pathway_146_2pc.xlsx\")\n",
    "clinical = pd.read_excel(\"/Users/alexandermollers/Documents/GitHub/survival_analysis/data/multiomics/Clinical.xlsx\")\n",
    "pathway_mask =pd.read_csv(\"/Users/alexandermollers/Documents/GitHub/survival_analysis/data/pathway_mask.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_pathway_mask = pathway_mask[pathway_mask[\"Unnamed: 0\"].str.contains(\"KEGG\")].copy()\n",
    "kegg_pathway_mask.rename(columns={'Unnamed: 0':'pathway'}, inplace=True)\n",
    "kegg_pathway_mask[\"pathway\"] = kegg_pathway_mask[\"pathway\"].str.replace(r'KEGG_','')\n",
    "cnn_pthws = ordered_pathways.set_index(\"TOLL_LIKE_RECEPTOR_SIGNALING_PATHWAY\").join(kegg_pathway_mask.set_index(\"pathway\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind,v = create_overlap_adjacency(cnn_pthws) #edge indexes for gnn, edge weights calculated based on pathway overlap\n",
    "prepared_data, age, outcomes = data_prep_pathcnn_paper(pca_exp,pca_cnv,pca_mt,clinical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(prepared_data)\n",
    "indices = np.arange(n_samples)\n",
    "X_train, X_test, y_train, y_test,train_ind,test_ind = train_test_split(prepared_data, outcomes,indices,\n",
    "                                                    stratify=outcomes, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation,train_ind,val_ind = train_test_split(X_train, y_train,train_ind,\n",
    "                                                    stratify=y_train, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "age_train = torch.tensor(age[train_ind])\n",
    "age_val = torch.tensor(age[val_ind])\n",
    "age_test = torch.tensor(age[test_ind])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "van_loader_params = {'batch_size': 64,\n",
    "          'shuffle': True}\n",
    "\n",
    "cnn_loader_params = {'batch_size': 64,\n",
    "          'shuffle': True}\n",
    "\n",
    "van_train_loader = prepare_vanilla_loader(X_train,age_train,y_train,van_loader_params)\n",
    "van_val_loader = prepare_vanilla_loader(X_validation,age_val,y_validation,van_loader_params)\n",
    "van_test_loader = prepare_vanilla_loader(X_test,age_test,y_test,van_loader_params)\n",
    "\n",
    "img_rows, img_cols = 146, 6\n",
    "cnn_train_loader = prepare_cnn_loader(X_train,age_train,y_train,img_rows,img_cols,cnn_loader_params)\n",
    "cnn_val_loader = prepare_cnn_loader(X_validation,age_val,y_validation,img_rows,img_cols,cnn_loader_params)\n",
    "cnn_test_loader = prepare_cnn_loader(X_test,age_test,y_test,img_rows,img_cols,cnn_loader_params)\n",
    "\n",
    "\n",
    "train_data_list = [Data(x = X_train[i],edge_index = ind,edge_weights = v, age = age_train[i], y = y_train[i]) for i in range(len(X_train))]\n",
    "val_data_list = [Data(x = X_validation[i],edge_index = ind,edge_weights = v, age = age_val[i], y = y_validation[i]) for i in range(len(X_validation))]\n",
    "test_data_list = [Data(x = X_test[i],edge_index = ind,edge_weights = v,age = age_test[i], y = y_test[i]) for i in range(len(X_test))]\n",
    "\n",
    "gnn_train_loader = torch_geometric.loader.DataLoader(train_data_list, batch_size=1)\n",
    "gnn_val_loader = torch_geometric.loader.DataLoader(val_data_list, batch_size=1)\n",
    "gnn_test_loader = torch_geometric.loader.DataLoader(test_data_list, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VanillaNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(876, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(65, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x,clinical_vars):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x_cat = torch.cat((x, clinical_vars.unsqueeze(-1)),1)\n",
    "        x = F.relu(self.fc3(x_cat))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        return x\n",
    "                          \n",
    "                          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_vanilla(model,train_loader,val_loader):\n",
    "    \n",
    "\n",
    "    # init model\n",
    "    model = cpath_md_lg(5567,860, 100, 30,args,mask = pathway_mask)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        # init optimizer\n",
    "    optimizer = optim.Adam(model.parameters(),lr=args.lr)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss_train_score, c_index_train_score,pll_train = train(args, model, cpath_train_loader, epoch, optimizer)\n",
    "        loss_val_score,c_index_val_score,pll_val = validate(args, cpath_val_loader, model, epoch)\n",
    "        test_conc_metric,test_pll = test(args, model, cpath_test_loader)\n",
    "\n",
    "        epoch_dict = {\n",
    "\n",
    "            'epoch': epoch + 1,\n",
    "            'lr': args.lr,\n",
    "            'gp_mean': args.gp_mean,\n",
    "            'gp_var': args.gp_var,\n",
    "            'loss_train_score': loss_train_score,\n",
    "            'train_pll': pll_train,\n",
    "            'ctrain_score': c_index_train_score,\n",
    "            'cval_score': c_index_val_score,\n",
    "            'loss_val_score': loss_val_score,\n",
    "            'val_pll': pll_val,\n",
    "            'test_conc_metric': test_conc_metric,\n",
    "            'test_pll': test_pll,\n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "        is_best = loss_val_score < best_loss_val_score\n",
    "        best_loss_val_score = min(loss_val_score, best_loss_val_score)\n",
    "        if args.save_best_model & is_best:\n",
    "                save_checkpoint(\n",
    "                    epoch_dict,\n",
    "                    is_best,\n",
    "                    filename=os.path.join(\n",
    "                        args.save_dir,\n",
    "                        'bayesian_{}.pth'.format(args.arch)))\n",
    "\n",
    "        epoch_dict.pop(\"state_dict\", None)\n",
    "        epoch_log = {k: [v] for k, v in epoch_dict.items()}\n",
    "        epoch_log_df = pd.DataFrame.from_dict(epoch_log, orient=\"columns\")\n",
    "        log_file_name = args.arch + '_logs.csv'\n",
    "        log_path = os.path.join(args.log_dir, log_file_name)\n",
    "        epoch_log_df.to_csv(log_path, mode='a', header=not os.path.exists(log_path),index = False)\n",
    "\n",
    "    return epoch_log_df.to_string(header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6361969111969111\n",
      "0.6247582205029014\n",
      "Epoch: 001, Train Acc: 0.4645, Test Acc: 0.4483\n",
      "0.6368725868725869\n",
      "0.5938104448742747\n",
      "Epoch: 002, Train Acc: 0.8361, Test Acc: 0.7931\n",
      "0.6402509652509653\n",
      "0.5938104448742747\n",
      "Epoch: 003, Train Acc: 0.8415, Test Acc: 0.7931\n",
      "0.6797297297297297\n",
      "0.5831721470019342\n",
      "Epoch: 004, Train Acc: 0.8525, Test Acc: 0.7759\n",
      "0.7233590733590733\n",
      "0.6982591876208898\n",
      "Epoch: 005, Train Acc: 0.8525, Test Acc: 0.7931\n",
      "0.8004826254826255\n",
      "0.6769825918762089\n",
      "Epoch: 006, Train Acc: 0.8361, Test Acc: 0.7586\n",
      "0.8482625482625482\n",
      "0.6663442940038684\n",
      "Epoch: 007, Train Acc: 0.8251, Test Acc: 0.7414\n",
      "0.8557915057915058\n",
      "0.7224371373307543\n",
      "Epoch: 008, Train Acc: 0.8197, Test Acc: 0.7759\n",
      "0.8659266409266408\n",
      "0.7117988394584138\n",
      "Epoch: 009, Train Acc: 0.8361, Test Acc: 0.7586\n",
      "0.903088803088803\n",
      "0.7224371373307543\n",
      "Epoch: 010, Train Acc: 0.8962, Test Acc: 0.7759\n",
      "0.9132239382239381\n",
      "0.6982591876208898\n",
      "Epoch: 011, Train Acc: 0.9126, Test Acc: 0.7931\n",
      "0.9132239382239381\n",
      "0.6876208897485493\n",
      "Epoch: 012, Train Acc: 0.9126, Test Acc: 0.7759\n",
      "0.9166023166023165\n",
      "0.6876208897485493\n",
      "Epoch: 013, Train Acc: 0.9180, Test Acc: 0.7759\n",
      "0.9166023166023165\n",
      "0.6769825918762089\n",
      "Epoch: 014, Train Acc: 0.9180, Test Acc: 0.7586\n",
      "0.9308880308880308\n",
      "0.6769825918762089\n",
      "Epoch: 015, Train Acc: 0.9235, Test Acc: 0.7586\n",
      "0.9342664092664092\n",
      "0.6876208897485493\n",
      "Epoch: 016, Train Acc: 0.9290, Test Acc: 0.7759\n",
      "0.972972972972973\n",
      "0.7088974854932302\n",
      "Epoch: 017, Train Acc: 0.9563, Test Acc: 0.8103\n",
      "0.9763513513513514\n",
      "0.6634429400386848\n",
      "Epoch: 018, Train Acc: 0.9617, Test Acc: 0.7931\n",
      "0.9831081081081081\n",
      "0.6634429400386848\n",
      "Epoch: 019, Train Acc: 0.9727, Test Acc: 0.7931\n",
      "0.9864864864864865\n",
      "0.6634429400386848\n",
      "Epoch: 020, Train Acc: 0.9781, Test Acc: 0.7931\n",
      "0.9864864864864865\n",
      "0.6740812379110251\n",
      "Epoch: 021, Train Acc: 0.9781, Test Acc: 0.8103\n",
      "0.9932432432432433\n",
      "0.6740812379110251\n",
      "Epoch: 022, Train Acc: 0.9891, Test Acc: 0.8103\n",
      "0.9966216216216216\n",
      "0.6740812379110251\n",
      "Epoch: 023, Train Acc: 0.9945, Test Acc: 0.8103\n",
      "0.9966216216216216\n",
      "0.6740812379110251\n",
      "Epoch: 024, Train Acc: 0.9945, Test Acc: 0.8103\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 025, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 026, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 027, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6740812379110251\n",
      "Epoch: 028, Train Acc: 1.0000, Test Acc: 0.8103\n",
      "1.0\n",
      "0.6740812379110251\n",
      "Epoch: 029, Train Acc: 1.0000, Test Acc: 0.8103\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 030, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 031, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 032, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 033, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 034, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6740812379110251\n",
      "Epoch: 035, Train Acc: 1.0000, Test Acc: 0.8103\n",
      "1.0\n",
      "0.6740812379110251\n",
      "Epoch: 036, Train Acc: 1.0000, Test Acc: 0.8103\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 037, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 038, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 039, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 040, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 041, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 042, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 043, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 044, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 045, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 046, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 047, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 048, Train Acc: 1.0000, Test Acc: 0.7931\n",
      "1.0\n",
      "0.6286266924564797\n",
      "Epoch: 049, Train Acc: 1.0000, Test Acc: 0.7931\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "from IPython.display import Javascript\n",
    "\n",
    "model = VanillaNN()\n",
    "model.double()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0006,betas=(0.9, 0.999))\n",
    "\n",
    "class_weights=[1,4.2]\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "\n",
    "def train_van(loader):\n",
    "    model.train()\n",
    "\n",
    "    for x,clin_vars,y in loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(x, clin_vars)  # Perform a single forward pass.\n",
    "         loss = criterion(out.float(), y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test_van(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     predicted = []\n",
    "     true_label = []\n",
    "     for x,clin_vars,y in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(x,clin_vars)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         predicted.extend(pred.detach().cpu().numpy().tolist())\n",
    "         true_label.extend(y.detach().cpu().numpy().tolist())\n",
    "         correct += int((pred == y).sum())  # Check against ground-truth labels.\n",
    "   \n",
    "     test_auc = roc_auc_score(true_label, predicted)\n",
    "     print(test_auc)\n",
    "    \n",
    "          \n",
    "        \n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 50):\n",
    "    train_van(van_train_loader)\n",
    "    train_acc = test_van(van_train_loader)\n",
    "    test_acc = test_van(van_test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global best_loss_val_score\n",
    "\n",
    "    pathway_mask = pd.read_csv(\"../data/pathway_mask.csv\", index_col=0).values\n",
    "    pathway_mask = torch.from_numpy(pathway_mask).type(torch.FloatTensor)\n",
    "    if args.cuda:\n",
    "        pathway_mask=pathway_mask.to(device = 'cuda')\n",
    "\n",
    "    train_data = pd.read_csv(\"../data/train.csv\")\n",
    "    X_train_np = train_data.drop([\"SAMPLE_ID\", \"OS_MONTHS\", \"OS_EVENT\", \"AGE\"], axis=1).values\n",
    "    tb_train = train_data.loc[:, [\"OS_MONTHS\"]].values\n",
    "    e_train = train_data.loc[:, [\"OS_EVENT\"]].values\n",
    "    clinical_vars_train = train_data.loc[:, [\"AGE\"]].values\n",
    "\n",
    "    val_data = pd.read_csv(\"../data/validation.csv\")\n",
    "    X_val_np = val_data.drop([\"SAMPLE_ID\", \"OS_MONTHS\", \"OS_EVENT\", \"AGE\"], axis=1).values\n",
    "    tb_val = val_data.loc[:, [\"OS_MONTHS\"]].values\n",
    "    e_val = val_data.loc[:, [\"OS_EVENT\"]].values\n",
    "    clinical_vars_val = val_data.loc[:, [\"AGE\"]].values\n",
    "\n",
    "    test_data = pd.read_csv(\"../data/test.csv\")\n",
    "    X_test_np = test_data.drop([\"SAMPLE_ID\", \"OS_MONTHS\", \"OS_EVENT\", \"AGE\"], axis=1).values\n",
    "    tb_test = test_data.loc[:, [\"OS_MONTHS\"]].values\n",
    "    e_test = test_data.loc[:, [\"OS_EVENT\"]].values\n",
    "    clinical_vars_test = test_data.loc[:, [\"AGE\"]].values\n",
    "\n",
    "    cpath_train_dataset = cpath_dataset(X_train_np,\n",
    "                                        clinical_vars_train,\n",
    "                                        tb_train,\n",
    "                                        e_train)\n",
    "\n",
    "    cpath_val_dataset = cpath_dataset(X_val_np,\n",
    "                                      clinical_vars_val,\n",
    "                                      tb_val,\n",
    "                                      e_val)\n",
    "    cpath_test_dataset = cpath_dataset(X_test_np,\n",
    "                                      clinical_vars_test,\n",
    "                                      tb_test,\n",
    "                                      e_test)\n",
    "\n",
    "    # import data\n",
    "    cpath_train_loader = torch.utils.data.DataLoader(cpath_train_dataset,\n",
    "                                                     batch_size=len(cpath_train_dataset),\n",
    "                                                     shuffle=True,\n",
    "                                                     num_workers=0)\n",
    "\n",
    "    cpath_val_loader = torch.utils.data.DataLoader(cpath_val_dataset,\n",
    "                                                   batch_size=len(cpath_val_dataset),\n",
    "                                                   shuffle=False,\n",
    "                                                   num_workers=0)\n",
    "    cpath_test_loader = torch.utils.data.DataLoader(cpath_test_dataset,\n",
    "                                                   batch_size=len(cpath_test_dataset),\n",
    "                                                   shuffle=False,\n",
    "                                                   num_workers=0)\n",
    "\n",
    "\n",
    "    # init model\n",
    "    model = cpath_md_lg(5567,860, 100, 30,args,mask = pathway_mask)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        # init optimizer\n",
    "    optimizer = optim.Adam(model.parameters(),lr=args.lr)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss_train_score, c_index_train_score,pll_train = train(args, model, cpath_train_loader, epoch, optimizer)\n",
    "        loss_val_score,c_index_val_score,pll_val = validate(args, cpath_val_loader, model, epoch)\n",
    "        test_conc_metric,test_pll = test(args, model, cpath_test_loader)\n",
    "\n",
    "        epoch_dict = {\n",
    "\n",
    "            'epoch': epoch + 1,\n",
    "            'lr': args.lr,\n",
    "            'gp_mean': args.gp_mean,\n",
    "            'gp_var': args.gp_var,\n",
    "            'loss_train_score': loss_train_score,\n",
    "            'train_pll': pll_train,\n",
    "            'ctrain_score': c_index_train_score,\n",
    "            'cval_score': c_index_val_score,\n",
    "            'loss_val_score': loss_val_score,\n",
    "            'val_pll': pll_val,\n",
    "            'test_conc_metric': test_conc_metric,\n",
    "            'test_pll': test_pll,\n",
    "            'state_dict': model.state_dict()\n",
    "        }\n",
    "        is_best = loss_val_score < best_loss_val_score\n",
    "        best_loss_val_score = min(loss_val_score, best_loss_val_score)\n",
    "        if args.save_best_model & is_best:\n",
    "                save_checkpoint(\n",
    "                    epoch_dict,\n",
    "                    is_best,\n",
    "                    filename=os.path.join(\n",
    "                        args.save_dir,\n",
    "                        'bayesian_{}.pth'.format(args.arch)))\n",
    "\n",
    "        epoch_dict.pop(\"state_dict\", None)\n",
    "        epoch_log = {k: [v] for k, v in epoch_dict.items()}\n",
    "        epoch_log_df = pd.DataFrame.from_dict(epoch_log, orient=\"columns\")\n",
    "        log_file_name = args.arch + '_logs.csv'\n",
    "        log_path = os.path.join(args.log_dir, log_file_name)\n",
    "        epoch_log_df.to_csv(log_path, mode='a', header=not os.path.exists(log_path),index = False)\n",
    "\n",
    "    return epoch_log_df.to_string(header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATH CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PathCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = (3,3),padding = \"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = (3,3),padding = \"same\")\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(6913, 64)  \n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.lin = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x,clinical_vars):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (4, 2))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.flat(x)\n",
    "        x_cat = torch.cat((x, clinical_vars.unsqueeze(-1)),1)\n",
    "        x = F.relu(self.fc1(x_cat))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "                          \n",
    "                          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PathCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=6913, out_features=64, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathcnn = PathCNN()\n",
    "pathcnn.double()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6024131274131274\n",
      "0.5473887814313347\n",
      "Epoch: 001, Train Acc: 0.4098, Test Acc: 0.3793\n",
      "0.6994208494208494\n",
      "0.6905222437137332\n",
      "Epoch: 002, Train Acc: 0.7432, Test Acc: 0.7241\n",
      "0.6745173745173745\n",
      "0.6179883945841393\n",
      "Epoch: 003, Train Acc: 0.8087, Test Acc: 0.7759\n",
      "0.7046332046332047\n",
      "0.6315280464216635\n",
      "Epoch: 004, Train Acc: 0.7869, Test Acc: 0.7414\n",
      "0.7708494208494208\n",
      "0.6663442940038684\n",
      "Epoch: 005, Train Acc: 0.7705, Test Acc: 0.7414\n",
      "0.7640926640926642\n",
      "0.6450676982591876\n",
      "Epoch: 006, Train Acc: 0.7596, Test Acc: 0.7069\n",
      "0.7716216216216216\n",
      "0.6344294003868471\n",
      "Epoch: 007, Train Acc: 0.7541, Test Acc: 0.6897\n",
      "0.7926640926640929\n",
      "0.6450676982591876\n",
      "Epoch: 008, Train Acc: 0.7705, Test Acc: 0.7069\n",
      "0.7828185328185329\n",
      "0.6315280464216635\n",
      "Epoch: 009, Train Acc: 0.8251, Test Acc: 0.7414\n",
      "0.797876447876448\n",
      "0.620889748549323\n",
      "Epoch: 010, Train Acc: 0.8142, Test Acc: 0.7241\n",
      "0.7716216216216216\n",
      "0.6450676982591876\n",
      "Epoch: 011, Train Acc: 0.7541, Test Acc: 0.7069\n",
      "0.7664092664092663\n",
      "0.6934235976789168\n",
      "Epoch: 012, Train Acc: 0.7104, Test Acc: 0.6724\n",
      "0.8012548262548262\n",
      "0.6102514506769826\n",
      "Epoch: 013, Train Acc: 0.8197, Test Acc: 0.7069\n",
      "0.7636100386100386\n",
      "0.5270793036750484\n",
      "Epoch: 014, Train Acc: 0.8470, Test Acc: 0.7414\n",
      "0.8147683397683398\n",
      "0.6421663442940039\n",
      "Epoch: 015, Train Acc: 0.8415, Test Acc: 0.7586\n",
      "0.8204633204633205\n",
      "0.6450676982591876\n",
      "Epoch: 016, Train Acc: 0.7978, Test Acc: 0.7069\n",
      "0.7495173745173745\n",
      "0.7147001934235977\n",
      "Epoch: 017, Train Acc: 0.6831, Test Acc: 0.7069\n",
      "0.8441119691119692\n",
      "0.655705996131528\n",
      "Epoch: 018, Train Acc: 0.8361, Test Acc: 0.7241\n",
      "0.8249034749034749\n",
      "0.6528046421663444\n",
      "Epoch: 019, Train Acc: 0.8579, Test Acc: 0.7759\n",
      "0.8249034749034749\n",
      "0.6528046421663444\n",
      "Epoch: 020, Train Acc: 0.8579, Test Acc: 0.7759\n",
      "0.8358108108108109\n",
      "0.6528046421663444\n",
      "Epoch: 021, Train Acc: 0.8579, Test Acc: 0.7759\n",
      "0.8441119691119692\n",
      "0.6663442940038684\n",
      "Epoch: 022, Train Acc: 0.8361, Test Acc: 0.7414\n",
      "0.8305984555984556\n",
      "0.7011605415860735\n",
      "Epoch: 023, Train Acc: 0.8142, Test Acc: 0.7414\n",
      "0.8441119691119692\n",
      "0.7117988394584138\n",
      "Epoch: 024, Train Acc: 0.8361, Test Acc: 0.7586\n",
      "0.8610038610038611\n",
      "0.6421663442940039\n",
      "Epoch: 025, Train Acc: 0.8634, Test Acc: 0.7586\n",
      "0.8677606177606179\n",
      "0.6528046421663444\n",
      "Epoch: 026, Train Acc: 0.8743, Test Acc: 0.7759\n",
      "0.8576254826254827\n",
      "0.6663442940038684\n",
      "Epoch: 027, Train Acc: 0.8579, Test Acc: 0.7414\n",
      "0.8373552123552124\n",
      "0.7011605415860735\n",
      "Epoch: 028, Train Acc: 0.8251, Test Acc: 0.7414\n",
      "0.8610038610038611\n",
      "0.6663442940038684\n",
      "Epoch: 029, Train Acc: 0.8634, Test Acc: 0.7414\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "\n",
    "model = PathCNN()\n",
    "model.double()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,betas=(0.9, 0.999))\n",
    "\n",
    "class_weights=[1,4.2]\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "\n",
    "def train_cnn(loader):\n",
    "    model.train()\n",
    "\n",
    "    for x,clin_vars,y in loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(x, clin_vars)  # Perform a single forward pass.\n",
    "         loss = criterion(out.float(), y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test_cnn(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     predicted = []\n",
    "     true_label = []\n",
    "     for x,clin_vars,y in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(x,clin_vars)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         predicted.extend(pred.detach().cpu().numpy().tolist())\n",
    "         true_label.extend(y.detach().cpu().numpy().tolist())\n",
    "         correct += int((pred == y).sum())  # Check against ground-truth labels.\n",
    "   \n",
    "     test_auc = roc_auc_score(true_label, predicted)\n",
    "     print(test_auc)\n",
    "    \n",
    "          \n",
    "        \n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    train_cnn(cnn_train_loader)\n",
    "    train_acc = test_cnn(cnn_train_loader)\n",
    "    test_acc = test_cnn(cnn_test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(6, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.dense = Linear(hidden_channels+1, hidden_channels+1)\n",
    "        self.lin = Linear(hidden_channels+1, 2)\n",
    "\n",
    "    def forward(self, x, edge_index,edge_weights,clinical_vars,batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index,edge_weights)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index,edge_weights)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index,edge_weights)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        \n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x_cat = torch.cat((x, clinical_vars.unsqueeze(1)),1)\n",
    "        x = self.lin(x_cat)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[1;32m      2\u001b[0m     testd \u001b[38;5;241m=\u001b[39m d\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for d in train_loader:\n",
    "    testd = d\n",
    "#model = GCN(hidden_channels=64)\n",
    "#model.double()\n",
    "#l = model(testd.x, testd.edge_index,testd.edge_weights,testd.age, testd.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "model.double()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "class_weights=compute_class_weight(class_weight = 'balanced',classes = np.unique(y_train),y = y_train.numpy())\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index,data.edge_weights,data.age, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out.float(), data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     predicted = []\n",
    "     true_label = []\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.edge_weights,data.age,data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         predicted.append(pred.detach().cpu().numpy().item())\n",
    "         true_label.append(data.y.detach().cpu().numpy().item())\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "   \n",
    "     test_auc = roc_auc_score(true_label, predicted)\n",
    "     print(test_auc)\n",
    "    \n",
    "          \n",
    "        \n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = GCNConv(6, hidden_channels)\n",
    "conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "lin = Linear(hidden_channels+1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1. Obtain node embeddings \n",
    "x = conv1(testd.x, testd.edge_index,testd.edge_weights)\n",
    "x = x.relu()\n",
    "x = conv2(x, testd.edge_index,testd.edge_weights)\n",
    "x = x.relu()\n",
    "x = conv3(x, testd.edge_index,testd.edge_weights)\n",
    "\n",
    "# 2. Readout layer\n",
    "x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testd.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(testd.x, testd.edge_index, testd.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtens = torch.squeeze(torch.tensor(testd.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(data.x, data.edge_index, data.batch)  # Perform a single forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t = fastConnectionDf(cnn_pthws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kegg_pathway_mask[\"pathway\"] = kegg_pathway_mask[\"pathway\"].str.replace(r'REACTOME_','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_pathway_mask[\"pathway\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_index(\"TOLL_LIKE_RECEPTOR_SIGNALING_PATHWAY\").join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(l).set_index(\"Unnamed: 0\").join(p.set_index(\"TOLL_LIKE_RECEPTOR_SIGNALING_PATHWAY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (survival_analysis)",
   "language": "python",
   "name": "pycharm-3638621"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
